{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Harvester Overview \u00b6 Harvester is an open-source hyper-converged infrastructure (HCI) software built on Kubernetes. It is an open alternative to using a proprietary HCI stack that incorporates the design and ethos of Cloud Native Computing . Harvester Features \u00b6 Harvester implements HCI on bare metal servers. Harvester is designed to use local, direct attached storage instead of complex external SANs. It ships as an integrated bootable appliance image that can be deployed directly to servers through an ISO or PXE boot artifact. Some notable features of Harvester include the following: VM lifecycle management including SSH-Key injection, cloud-init, and graphic and serial port console VM live migration support Supported VM backup and restore Distributed block storage Multiple network interface controllers (NICs) in the VM connecting to the management network or VLANs Virtual Machine and cloud-init templates Rancher integration with multi-cluster management and the Harvester node driver PXE/iPXE boot support Virtual IP and bond NIC support Monitoring integration Harvester Architecture \u00b6 The following diagram outlines a high-level architecture of Harvester: Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes. KubeVirt is a virtual machine management add-on for Kubernetes. Elemental for openSUSE Leap 15.3 is a Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster. Hardware Requirements \u00b6 To get the Harvester server up and running, the following minimum hardware is required: Type Requirements CPU x86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum; 16-core or above preferred Memory 32 GB minimum; 64 GB or above preferred Disk Capacity 140 GB minimum; 500 GB or above preferred Disk Performance 5,000+ random IOPS per disk (SSD/NVMe). Management nodes (first three nodes) must be fast enough for etcd . Network Card 1 Gbps Ethernet minimum; 10Gbps Ethernet recommended Network Switch Trunking of ports required for VLAN support Quick start \u00b6 You can install Harvester via the ISO installation or the PXE boot installation. Instructions are provided in the sections below. ISO Installation \u00b6 You can use the ISO to install Harvester directly on the bare metal server to form a Harvester cluster. Users can add one or many compute nodes to join the existing cluster. To get the Harvester ISO, download it from the Github releases . During the installation, you can either choose to form a new cluster or join the node to an existing cluster. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer . Choose the installation mode by either creating a new Harvester cluster or by joining an existing one. Choose the installation device on which the Harvester cluster will be installed Note: By default, Harvester uses GPT partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select MBR . Configure the hostname and select the network interface for the management network. By default, Harvester will create a bond NIC named harvester-mgmt , and the IP address can either be configured via DHCP or by static method. Optional: Configure the DNS servers; use commas as delimiters. Configure the Virtual IP which you can use to access the cluster or join other nodes to the cluster (Note: If your IP address is configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP) . Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default SSH user is rancher . (Optional) Configure the NTP Servers of the node. This defaults to 0.suse.pool.ntp.org . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here. Otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote server URL. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to customize the host with a Harvester configuration file, enter the HTTP URL here. Confirm the installation options and Harvester will be installed to your host. The installation may take a few minutes to complete. Once the installation is complete, the host will restart, and a console UI with management URL and status will be displayed. (You can Use F12 to switch between the Harvester console and the Shell). The default URL of the web interface is https://your-virtual-ip . Users will be prompted to set the password for the default admin user at first login. PXE/iPXE Installation \u00b6 Harvester can also be installed automatically. Please refer to PXE Boot Install for detailed instructions and additional guidance. Note More iPXE usage examples are available at harvester/ipxe-examples .","title":"Harvester Overview"},{"location":"#harvester-overview","text":"Harvester is an open-source hyper-converged infrastructure (HCI) software built on Kubernetes. It is an open alternative to using a proprietary HCI stack that incorporates the design and ethos of Cloud Native Computing .","title":"Harvester Overview"},{"location":"#harvester-features","text":"Harvester implements HCI on bare metal servers. Harvester is designed to use local, direct attached storage instead of complex external SANs. It ships as an integrated bootable appliance image that can be deployed directly to servers through an ISO or PXE boot artifact. Some notable features of Harvester include the following: VM lifecycle management including SSH-Key injection, cloud-init, and graphic and serial port console VM live migration support Supported VM backup and restore Distributed block storage Multiple network interface controllers (NICs) in the VM connecting to the management network or VLANs Virtual Machine and cloud-init templates Rancher integration with multi-cluster management and the Harvester node driver PXE/iPXE boot support Virtual IP and bond NIC support Monitoring integration","title":"Harvester Features"},{"location":"#harvester-architecture","text":"The following diagram outlines a high-level architecture of Harvester: Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes. KubeVirt is a virtual machine management add-on for Kubernetes. Elemental for openSUSE Leap 15.3 is a Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster.","title":"Harvester Architecture"},{"location":"#hardware-requirements","text":"To get the Harvester server up and running, the following minimum hardware is required: Type Requirements CPU x86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum; 16-core or above preferred Memory 32 GB minimum; 64 GB or above preferred Disk Capacity 140 GB minimum; 500 GB or above preferred Disk Performance 5,000+ random IOPS per disk (SSD/NVMe). Management nodes (first three nodes) must be fast enough for etcd . Network Card 1 Gbps Ethernet minimum; 10Gbps Ethernet recommended Network Switch Trunking of ports required for VLAN support","title":"Hardware Requirements"},{"location":"#quick-start","text":"You can install Harvester via the ISO installation or the PXE boot installation. Instructions are provided in the sections below.","title":"Quick start"},{"location":"#iso-installation","text":"You can use the ISO to install Harvester directly on the bare metal server to form a Harvester cluster. Users can add one or many compute nodes to join the existing cluster. To get the Harvester ISO, download it from the Github releases . During the installation, you can either choose to form a new cluster or join the node to an existing cluster. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer . Choose the installation mode by either creating a new Harvester cluster or by joining an existing one. Choose the installation device on which the Harvester cluster will be installed Note: By default, Harvester uses GPT partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select MBR . Configure the hostname and select the network interface for the management network. By default, Harvester will create a bond NIC named harvester-mgmt , and the IP address can either be configured via DHCP or by static method. Optional: Configure the DNS servers; use commas as delimiters. Configure the Virtual IP which you can use to access the cluster or join other nodes to the cluster (Note: If your IP address is configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP) . Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default SSH user is rancher . (Optional) Configure the NTP Servers of the node. This defaults to 0.suse.pool.ntp.org . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here. Otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote server URL. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to customize the host with a Harvester configuration file, enter the HTTP URL here. Confirm the installation options and Harvester will be installed to your host. The installation may take a few minutes to complete. Once the installation is complete, the host will restart, and a console UI with management URL and status will be displayed. (You can Use F12 to switch between the Harvester console and the Shell). The default URL of the web interface is https://your-virtual-ip . Users will be prompted to set the password for the default admin user at first login.","title":"ISO Installation"},{"location":"#pxeipxe-installation","text":"Harvester can also be installed automatically. Please refer to PXE Boot Install for detailed instructions and additional guidance. Note More iPXE usage examples are available at harvester/ipxe-examples .","title":"PXE/iPXE Installation"},{"location":"airgap/","text":"Air Gapped Environment \u00b6 This section describes how to use Harvester in an air gapped environment. Some use cases could be where Harvester will be installed offline, behind a firewall, or behind a proxy. The Harvester ISO image contains all the packages to make it work in an air gapped environment. Working Behind an HTTP Proxy \u00b6 In some environments, the connection to external services, from the servers or VMs, requires an HTTP(S) proxy. Configure an HTTP Proxy During Installation \u00b6 You can configure the HTTP(S) proxy during the ISO installation as shown in picture below: Configure an HTTP Proxy in Harvester Settings \u00b6 You can configure the HTTP(S) proxy in the settings page of the Harvester dashboard: Go to the settings page of the Harvester UI. Find the http-proxy setting, click \u22ee > Edit setting Enter the value(s) for http-proxy , https-proxy and no-proxy . Note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,cattle-system.svc,.svc,.cluster.local","title":"Air Gapped Environment"},{"location":"airgap/#air-gapped-environment","text":"This section describes how to use Harvester in an air gapped environment. Some use cases could be where Harvester will be installed offline, behind a firewall, or behind a proxy. The Harvester ISO image contains all the packages to make it work in an air gapped environment.","title":"Air Gapped Environment"},{"location":"airgap/#working-behind-an-http-proxy","text":"In some environments, the connection to external services, from the servers or VMs, requires an HTTP(S) proxy.","title":"Working Behind an HTTP Proxy"},{"location":"airgap/#configure-an-http-proxy-during-installation","text":"You can configure the HTTP(S) proxy during the ISO installation as shown in picture below:","title":"Configure an HTTP Proxy During Installation"},{"location":"airgap/#configure-an-http-proxy-in-harvester-settings","text":"You can configure the HTTP(S) proxy in the settings page of the Harvester dashboard: Go to the settings page of the Harvester UI. Find the http-proxy setting, click \u22ee > Edit setting Enter the value(s) for http-proxy , https-proxy and no-proxy . Note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,cattle-system.svc,.svc,.cluster.local","title":"Configure an HTTP Proxy in Harvester Settings"},{"location":"authentication/","text":"Authentication \u00b6 After installation, user will be prompted to set the password for the default admin user on the first-time login. Note In the single cluster mode, only one default admin user is provided. Check out the Rancher Integration for multi-tenant management.","title":"Authentication"},{"location":"authentication/#authentication","text":"After installation, user will be prompted to set the password for the default admin user on the first-time login. Note In the single cluster mode, only one default admin user is provided. Check out the Rancher Integration for multi-tenant management.","title":"Authentication"},{"location":"faq/","text":"FAQ \u00b6 This FAQ is a work in progress designed to answer the questions our users most frequently ask about Harvester. How can I ssh login to the Harvester node? \u00b6 $ ssh rancher@node-ip What is the default login username and password of the Harvester dashboard? \u00b6 username: admin password: # you will be promoted to set the default password when logging in for the first time How can I access the kubeconfig file of the Harvester cluster? \u00b6 Option 1. You can download the kubeconfig file from the support page of the Harvester dashboard. Option 2. You can get the kubeconfig file from one of the Harvester management nodes. E.g., $ cat /etc/rancher/rke2/rke2.yaml How to install the qemu-guest-agent of a running VM? \u00b6 # cloud-init will only be executed once, reboot it after add the cloud-init config with the following command. $ cloud-init clean --logs --reboot https://cloudinit.readthedocs.io/en/latest/topics/cli.html#clean How can I reset the administrator password? \u00b6 In case you forget the administrator password, you can reset it via the command line. SSH to one of the management node and run the following command: # switch to root and run $ kubectl -n cattle-system exec $( kubectl --kubeconfig $KUBECONFIG -n cattle-system get pods -l app = rancher --no-headers | head -1 | awk '{ print $1 }' ) -c rancher -- reset-password New password for default administrator ( user-xxxxx ) : <new_password>","title":"FAQ"},{"location":"faq/#faq","text":"This FAQ is a work in progress designed to answer the questions our users most frequently ask about Harvester.","title":"FAQ"},{"location":"faq/#how-can-i-ssh-login-to-the-harvester-node","text":"$ ssh rancher@node-ip","title":"How can I ssh login to the Harvester node?"},{"location":"faq/#what-is-the-default-login-username-and-password-of-the-harvester-dashboard","text":"username: admin password: # you will be promoted to set the default password when logging in for the first time","title":"What is the default login username and password of the Harvester dashboard?"},{"location":"faq/#how-can-i-access-the-kubeconfig-file-of-the-harvester-cluster","text":"Option 1. You can download the kubeconfig file from the support page of the Harvester dashboard. Option 2. You can get the kubeconfig file from one of the Harvester management nodes. E.g., $ cat /etc/rancher/rke2/rke2.yaml","title":"How can I access the kubeconfig file of the Harvester cluster?"},{"location":"faq/#how-to-install-the-qemu-guest-agent-of-a-running-vm","text":"# cloud-init will only be executed once, reboot it after add the cloud-init config with the following command. $ cloud-init clean --logs --reboot https://cloudinit.readthedocs.io/en/latest/topics/cli.html#clean","title":"How to install the qemu-guest-agent of a running VM?"},{"location":"faq/#how-can-i-reset-the-administrator-password","text":"In case you forget the administrator password, you can reset it via the command line. SSH to one of the management node and run the following command: # switch to root and run $ kubectl -n cattle-system exec $( kubectl --kubeconfig $KUBECONFIG -n cattle-system get pods -l app = rancher --no-headers | head -1 | awk '{ print $1 }' ) -c rancher -- reset-password New password for default administrator ( user-xxxxx ) : <new_password>","title":"How can I reset the administrator password?"},{"location":"upgrade/","text":"Upgrading Harvester \u00b6 Note Upgrade is not supported from previous versions to v0.3.0 version. A manual upgrade process starting with v0.3.0 is being investigated. Harvester will inform the community once this process is in place. One-click upgrade will be supported starting with the v1.0.0 release.","title":"Upgrading Harvester"},{"location":"upgrade/#upgrading-harvester","text":"Note Upgrade is not supported from previous versions to v0.3.0 version. A manual upgrade process starting with v0.3.0 is being investigated. Harvester will inform the community once this process is in place. One-click upgrade will be supported starting with the v1.0.0 release.","title":"Upgrading Harvester"},{"location":"upload-image/","text":"Upload Images \u00b6 Currently, there are three ways that are supported to create an image: uploading images via URL, uploading images via local files, and creating images via volumes. Upload Images via URL \u00b6 To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Description and labels are optional. Note The image name will be auto-filled using the URL address's filename. You can customize the image name at any time. Upload Images via Local File \u00b6 Currently, qcow2, raw, and ISO images are supported. Note Please do not refresh the page until the file upload is finished. Create Images via Volumes \u00b6 On the Volumes page, click Export Image . Enter image name to create image.","title":"Upload Images"},{"location":"upload-image/#upload-images","text":"Currently, there are three ways that are supported to create an image: uploading images via URL, uploading images via local files, and creating images via volumes.","title":"Upload Images"},{"location":"upload-image/#upload-images-via-url","text":"To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Description and labels are optional. Note The image name will be auto-filled using the URL address's filename. You can customize the image name at any time.","title":"Upload Images via URL"},{"location":"upload-image/#upload-images-via-local-file","text":"Currently, qcow2, raw, and ISO images are supported. Note Please do not refresh the page until the file upload is finished.","title":"Upload Images via Local File"},{"location":"upload-image/#create-images-via-volumes","text":"On the Volumes page, click Export Image . Enter image name to create image.","title":"Create Images via Volumes"},{"location":"dev/dev-mode/","text":"Developer Mode Installation \u00b6 Developer mode (dev mode) is intended to be used for testing and development purposes. This video shows the dev mode installation. Requirements \u00b6 Multus is installed across your cluster and a corresponding NetworkAttachmentDefinition CRD is created. The Kubernetes node must have hardware virtualization support. To validate the support, use this command: cat /proc/cpuinfo | grep vmx - If you are using an RKE cluster, ipv4.ip_forward must be enabled for the CNI plugin in order for pod network to work as expected. Installation \u00b6 Harvester can be installed on a Kubernetes cluster in the following ways: Using the Helm CLI. As a Rancher catalog app, in which case the harvester/harvester repo is added to the Rancher Catalog as a Helm v3 app Please refer to the Harvester Helm chart for more details on installing and configuring the Helm chart. Option 1: Install using Helm \u00b6 Clone the GitHub repository: git clone https://github.com/harvester/harvester.git --depth = 1 Go to the Helm chart: cd harvester/deploy/charts Install the Harvester chart with the following commands: ### To install the chart with the release name `harvester`: ## Create the target namespace kubectl create ns harvester-system ## Install the chart to the target namespace helm install harvester harvester \\ --namespace harvester-system \\ --set longhorn.enabled = true,minio.persistence.storageClass = longhorn Option 2: Install using Rancher \u00b6 Tip You can create a test Kubernetes environment in Rancher using DigitalOcean as cloud provider. For details, see this section . Add the Harvester repo https://github.com/harvester/harvester to your Rancher catalogs by clicking Global > Tools > Catalogs . Specify the URL and name. Set the branch to stable if you need a stable release version. Set the Helm version to be Helm v3 . Click Create . Navigate to your project-level Apps . Click Launch and choose the Harvester app. (Optional) You can modify the configurations if needed. Otherwise, use the default options. Click Launch and wait for the app's components to be ready. Click the /index.html link to navigate to the Harvester UI, as shown in the figure below: DigitalOcean Test Environment \u00b6 You can create a test Kubernetes environment in Rancher using DigitalOcean as a cloud provider, which supports nested virtualization. We recommend using a 8 core, 16 GB RAM droplet, which will have nested virtualization enabled by default. This screenshot shows how to create a Rancher node template that would allow Rancher to provision such a node in DigitalOcean: For more information on how to launch DigitalOcean nodes with Rancher, refer to the Rancher documentation.","title":"Developer Mode Installation"},{"location":"dev/dev-mode/#developer-mode-installation","text":"Developer mode (dev mode) is intended to be used for testing and development purposes. This video shows the dev mode installation.","title":"Developer Mode Installation"},{"location":"dev/dev-mode/#requirements","text":"Multus is installed across your cluster and a corresponding NetworkAttachmentDefinition CRD is created. The Kubernetes node must have hardware virtualization support. To validate the support, use this command: cat /proc/cpuinfo | grep vmx - If you are using an RKE cluster, ipv4.ip_forward must be enabled for the CNI plugin in order for pod network to work as expected.","title":"Requirements"},{"location":"dev/dev-mode/#installation","text":"Harvester can be installed on a Kubernetes cluster in the following ways: Using the Helm CLI. As a Rancher catalog app, in which case the harvester/harvester repo is added to the Rancher Catalog as a Helm v3 app Please refer to the Harvester Helm chart for more details on installing and configuring the Helm chart.","title":"Installation"},{"location":"dev/dev-mode/#option-1-install-using-helm","text":"Clone the GitHub repository: git clone https://github.com/harvester/harvester.git --depth = 1 Go to the Helm chart: cd harvester/deploy/charts Install the Harvester chart with the following commands: ### To install the chart with the release name `harvester`: ## Create the target namespace kubectl create ns harvester-system ## Install the chart to the target namespace helm install harvester harvester \\ --namespace harvester-system \\ --set longhorn.enabled = true,minio.persistence.storageClass = longhorn","title":"Option 1: Install using Helm"},{"location":"dev/dev-mode/#option-2-install-using-rancher","text":"Tip You can create a test Kubernetes environment in Rancher using DigitalOcean as cloud provider. For details, see this section . Add the Harvester repo https://github.com/harvester/harvester to your Rancher catalogs by clicking Global > Tools > Catalogs . Specify the URL and name. Set the branch to stable if you need a stable release version. Set the Helm version to be Helm v3 . Click Create . Navigate to your project-level Apps . Click Launch and choose the Harvester app. (Optional) You can modify the configurations if needed. Otherwise, use the default options. Click Launch and wait for the app's components to be ready. Click the /index.html link to navigate to the Harvester UI, as shown in the figure below:","title":"Option 2: Install using Rancher"},{"location":"dev/dev-mode/#digitalocean-test-environment","text":"You can create a test Kubernetes environment in Rancher using DigitalOcean as a cloud provider, which supports nested virtualization. We recommend using a 8 core, 16 GB RAM droplet, which will have nested virtualization enabled by default. This screenshot shows how to create a Rancher node template that would allow Rancher to provision such a node in DigitalOcean: For more information on how to launch DigitalOcean nodes with Rancher, refer to the Rancher documentation.","title":"DigitalOcean Test Environment"},{"location":"host/host/","text":"Host Management \u00b6 Users can view and manage Harvester nodes from the host page. The first node always defaults to be a management node of the cluster. When there are more than three nodes, the two other nodes that first joined are automatically promoted to management nodes to form a HA cluster. Note Because Harvester is built on top of Kubernetes and uses etcd as its database, the maximum node fault toleration is one when there are three management nodes. Node Maintenance \u00b6 For admin users, you can click Enable Maintenance Mode to evict all VMs from a node automatically. It will leverage the VM live migration feature to migrate all VMs to other nodes automatically. Note that at least two active nodes are required to use this feature. Cordoning a Node \u00b6 Cordoning a node marks it as unschedulable. This feature is useful for performing short tasks on the node during small maintenance windows, like reboots, upgrades, or decommissions. When you\u2019re done, power back on and make the node schedulable again by uncordoning it. Deleting a Node \u00b6 Deleting a node is done in two phases: Delete the node from Harvester Go to the Hosts page On the node you want to modify, click \u22ee > Delete Uninstall RKE2 from the node Login to the node as root Run rke2-uninstall.sh to delete the whole RKE2 service. Warning You will lose all data of the control plane node after deleing the RKE2 service. Note There's a known issue about node hard delete. Once resolved, the last step can be skipped. Multi-disk Management - Tech Preview \u00b6 Users can view and add multiple disks as additional data volumes from the host detail page. Go to the Hosts page. On the node you want to modify, click \u22ee > Edit Config . Select the Disks tab and click Add Disks . Select either an additional raw block device or partition to add as an additional data volume. The Force Formatted option is required when adding an entire raw block device to form a single root disk partition using the ext4 filesystem. The Force Formatted option is optional when adding partitions where the filesystem type is ext4 , XFS or cannot be found. It is required when adding partitions of any other filesystem type.","title":"Host Management"},{"location":"host/host/#host-management","text":"Users can view and manage Harvester nodes from the host page. The first node always defaults to be a management node of the cluster. When there are more than three nodes, the two other nodes that first joined are automatically promoted to management nodes to form a HA cluster. Note Because Harvester is built on top of Kubernetes and uses etcd as its database, the maximum node fault toleration is one when there are three management nodes.","title":"Host Management"},{"location":"host/host/#node-maintenance","text":"For admin users, you can click Enable Maintenance Mode to evict all VMs from a node automatically. It will leverage the VM live migration feature to migrate all VMs to other nodes automatically. Note that at least two active nodes are required to use this feature.","title":"Node Maintenance"},{"location":"host/host/#cordoning-a-node","text":"Cordoning a node marks it as unschedulable. This feature is useful for performing short tasks on the node during small maintenance windows, like reboots, upgrades, or decommissions. When you\u2019re done, power back on and make the node schedulable again by uncordoning it.","title":"Cordoning a Node"},{"location":"host/host/#deleting-a-node","text":"Deleting a node is done in two phases: Delete the node from Harvester Go to the Hosts page On the node you want to modify, click \u22ee > Delete Uninstall RKE2 from the node Login to the node as root Run rke2-uninstall.sh to delete the whole RKE2 service. Warning You will lose all data of the control plane node after deleing the RKE2 service. Note There's a known issue about node hard delete. Once resolved, the last step can be skipped.","title":"Deleting a Node"},{"location":"host/host/#multi-disk-management-tech-preview","text":"Users can view and add multiple disks as additional data volumes from the host detail page. Go to the Hosts page. On the node you want to modify, click \u22ee > Edit Config . Select the Disks tab and click Add Disks . Select either an additional raw block device or partition to add as an additional data volume. The Force Formatted option is required when adding an entire raw block device to form a single root disk partition using the ext4 filesystem. The Force Formatted option is optional when adding partitions where the filesystem type is ext4 , XFS or cannot be found. It is required when adding partitions of any other filesystem type.","title":"Multi-disk Management - Tech Preview"},{"location":"install/harvester-configuration/","text":"Harvester Configuration \u00b6 Configuration Example \u00b6 Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is a configuration example: server_url : https://someserver:8443 token : TOKEN_VALUE os : ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username write_files : - encoding : \"\" content : test content owner : root path : /etc/test.txt permissions : '0755' hostname : myhost modules : - kvm - nvme sysctls : kernel.printk : \"4 4 1 7\" kernel.kptr_restrict : \"1\" dns_nameservers : - 8.8.8.8 - 1.1.1.1 ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org password : rancher environment : http_proxy : http://myserver https_proxy : http://myserver labels : foo : bar mylabel : myvalue install : mode : create networks : harvester-mgmt : interfaces : - name : ens5 method : dhcp force_efi : true device : /dev/vda silent : true iso_url : http://myserver/test.iso poweroff : true no_format : true debug : true tty : ttyS0 vip : 10.10.0.19 vip_hw_addr : 52:54:00:ec:0e:0b vip_mode : dhcp force_mbr : false no_data_partition : false system_settings : auto-disk-provision-paths : \"\" Configuration Reference \u00b6 Below is a reference of all configuration keys. Warning Security Risks : The configuration file contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. server_url \u00b6 Definition \u00b6 The URL of the Harvester server to join as an agent. This configuration is mandatory when the installation is in JOIN mode. It tells the Harvester installer where the main server is. Note To ensure a high availability (HA) Harvester cluster, either use the Harvester main server VIP or a domain name in server_url . Example \u00b6 server_url : https://someserver:8443 install : mode : join token \u00b6 Definition \u00b6 The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match what the server has. Example \u00b6 token : myclustersecret Or a node token token : \"K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4\" os.ssh_authorized_keys \u00b6 Definition \u00b6 A list of SSH authorized keys that should be added to the default user, rancher . SSH keys can be obtained from GitHub user accounts by using the format github:${USERNAME} . This is done by downloading the keys from https://github.com/${USERNAME}.keys . Example \u00b6 os : ssh_authorized_keys : - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D\" - \"github:ibuildthecloud\" os.write_files \u00b6 A list of files to write to disk on boot. The encoding field specifies the content's encoding. Valid encoding values are: \"\" : content data are written in plain text. In this case, the encoding field can be also omitted. b64 , base64 : content data are base64-encoded. gz , gzip : content data are gzip-compressed. gz+base64 , gzip+base64 , gz+b64 , gzip+b64 : content data are gzip-compressed first and then base64-encoded. Example os : write_files : - encoding : b64 content : CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4... owner : root:root path : /etc/connman/main.conf permissions : '0644' - content : | # My new /etc/sysconfig/samba file SMDBOPTIONS=\"-D\" path : /etc/sysconfig/samba - content : !!binary | f0VMRgIBAQAAAAAAAAAAAAIAPgABAAAAwARAAAAAAABAAAAAAAAAAJAVAAAAAA AEAAHgAdAAYAAAAFAAAAQAAAAAAAAABAAEAAAAAAAEAAQAAAAAAAwAEAAAAAAA AAAAAAAAAwAAAAQAAAAAAgAAAAAAAAACQAAAAAAAAAJAAAAAAAAcAAAAAAAAAB ... path : /bin/arch permissions : '0555' - content : | 15 * * * * root ship_logs path : /etc/crontab os.hostname \u00b6 Definition \u00b6 Set the system hostname. If the system hostname is supplied via DHCP, then that value will be used here. If this value is not set and one is not supplied via DHCP, then a random hostname will be generated. Example \u00b6 os : hostname : myhostname os.modules \u00b6 Definition \u00b6 A list of kernel modules to be loaded on start. Example \u00b6 os : modules : - kvm - nvme os.sysctls \u00b6 Definition \u00b6 Kernel sysctl to set up on start. These are the typical configurations found in /etc/sysctl.conf . Values must be specified as strings. Example \u00b6 os : sysctls : kernel.printk : 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict : \"1\" # force the YAML parser to read as a string os.dns_nameservers \u00b6 Definition \u00b6 Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS. Example \u00b6 os : dns_nameservers : - 8.8.8.8 - 1.1.1.1 os.ntp_servers \u00b6 Definition \u00b6 Fallback ntp servers to use if NTP is not configured elsewhere in the OS. Example \u00b6 os : ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org os.password \u00b6 Definition \u00b6 The password for the default user, rancher . By default, there is no password for the rancher user. If you set a password at runtime it will be reset on the next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to change your password on a Linux system and copy the value of the second field from /etc/shadow . You can also encrypt a password using openssl passwd -1 . Example \u00b6 Encrypted: os : password : \"$1$tYtghCfK$QHa51MS6MVAcfUKuOzNKt0\" Or clear text: os : password : supersecure os.environment \u00b6 Definition \u00b6 Environment variables to be set on K3s and other processes like the boot process. Primary use of this field is to set the HTTP proxy. Example \u00b6 os : environment : http_proxy : http://myserver https_proxy : http://myserver Note This example sets the HTTP(S) proxy for foundational OS components . To set up an HTTP(S) proxy for Harvester components such as fetching external images and backup to S3 services, see Settings/http-proxy . os.labels \u00b6 Definition \u00b6 Labels to be added to this Node. install.mode \u00b6 Definition \u00b6 Harvester installation mode: create : Creating a new Harvester installation. join : Join an existing Harvester installation. Need to specify server_url . Example \u00b6 install : mode : create install.networks \u00b6 Definition \u00b6 Configure network interfaces for the host machine. Each key-value pair represents a network interface. The key name becomes the network name, and the values are configurations for each network. Valid configuration fields are: method : Method to assign an IP to this network. The following are supported: static : Manually assign an IP and gateway. dhcp : Request an IP from the DHCP server. none : Do nothing. Useful when the interface does not need an IP, such as when creating VLAN network NIC in Harvester. ip : Static IP for this network. Required if static method is chosen. subnet_mask : Subnet mask for this network. Required if static method is chosen. gateway : Gateway for this network. Required if static method is chosen. interfaces : An array of interface names. If provided, the installer then combines these NICs into a single logical bonded interface. interfaces.name : The name of the slave interface for the bonded network. bond_options : Options for bonded interfaces. Refer to here for more info. If not provided, the following options would be used: mode: balance-tlb miimon: 100 Note A network called harvester-mgmt is mandatory to establish a valid management network . Note Harvester uses the systemd net naming scheme . Please make sure the interface name is present on the target machine before installation. Example \u00b6 install : mode : create networks : harvester-mgmt : # The management bond name. This is mandatory. interfaces : - name : ens5 method : dhcp bond_options : mode : balance-tlb miimon : 100 harvester-vlan : # The VLAN network bond name. User can then input `harvester-vlan` in the VLAN NIC setting in the GUI. interfaces : - name : ens6 method : none bond_options : mode : balance-tlb miimon : 100 bond0 : interfaces : - name : ens8 method : static ip : 10.10.18.2 subnet_mask : 255.255.255.0 gateway : 192.168.11.1 install.force_efi \u00b6 Force EFI installation even when EFI is not detected. Default: false . install.device \u00b6 The device to install the OS. install.silent \u00b6 Reserved. install.iso_url \u00b6 ISO to download and install from if booting from kernel/vmlinuz and not ISO. install.poweroff \u00b6 Shutdown the machine after installation instead of rebooting install.no_format \u00b6 Do not partition and format, assume layout exists already. install.debug \u00b6 Run the installation with additional logging and debugging enabled for the installed system. install.tty \u00b6 Definition \u00b6 The tty device used for the console. Example \u00b6 install : tty : ttyS0,115200n8 install.vip \u00b6 install.vip_mode \u00b6 install.vip_hw_addr \u00b6 Definition \u00b6 install.vip : The VIP of the Harvester management endpoint. After installation, users can access the Harvester GUI at the URL https://<VIP> . install.vip_mode dhcp : Harvester will send DHCP requests to get the VIP. The install.vip_hw_addr field needs to be provided. static : Harvester uses a static VIP. install.vip_hw_addr : The hardware address corresponding to the VIP. Users must configure their on-premise DHCP server to offer the configured VIP. The field is mandatory when install.vip_mode is dhcp . See Management Address for more information. Example \u00b6 Configure a static VIP. install : vip : 192.168.0.100 vip_mode : static Configure a DHCP VIP. install : vip : 10.10.0.19 vip_mode : dhcp vip_hw_addr : 52:54:00:ec:0e:0b install.force_mbr \u00b6 Definition \u00b6 By default, Harvester uses GPT partitioning scheme on both UEFI and BIOS systems. However, if you face compatibility issues, the MBR partitioning scheme can be forced on BIOS systems. Note Harvester creates an additional partition for storing VM data by default. When force using MBR, install.no_data_partition will be forced to true . In other words, no additional partition will be created and VM data will be stored in a partition shared with the OS data. Example \u00b6 install : force_mbr : true install.no_data_partition \u00b6 Definition \u00b6 Do not create an additional disk partition for storing VM data. An OS partition will then be used to store VM data. This is useful when you want to use additional disks to store VM data with the auto-disk-provision-paths setting. Default: false . Warning If VM data is stored in the OS partition and you have created too many VMs, there is a high chance of causing OS to malfunction due to the lack of disk space. Example \u00b6 install : no_data_partition : true system_settings \u00b6 Definition \u00b6 You can overwrite the default Harvester system settings by configuring system_settings . See the Settings page for additional information and the list of all the options. Note Overwriting system settings only works when Harvester is installed in \"create\" mode. If you install Harvester in \"join\" mode, this setting is ignored. Installing in \"join\" mode will adopt the system settings from the existing Harvester system. Example \u00b6 The example below overwrites http-proxy and ui-source settings. The values must be a string . system_settings : http-proxy : '{\"httpProxy\": \"http://my.proxy\", \"httpsProxy\": \"https://my.proxy\", \"noProxy\": \"some.internal.svc\"}' ui-source : auto","title":"Harvester Configuration"},{"location":"install/harvester-configuration/#harvester-configuration","text":"","title":"Harvester Configuration"},{"location":"install/harvester-configuration/#configuration-example","text":"Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is a configuration example: server_url : https://someserver:8443 token : TOKEN_VALUE os : ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username write_files : - encoding : \"\" content : test content owner : root path : /etc/test.txt permissions : '0755' hostname : myhost modules : - kvm - nvme sysctls : kernel.printk : \"4 4 1 7\" kernel.kptr_restrict : \"1\" dns_nameservers : - 8.8.8.8 - 1.1.1.1 ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org password : rancher environment : http_proxy : http://myserver https_proxy : http://myserver labels : foo : bar mylabel : myvalue install : mode : create networks : harvester-mgmt : interfaces : - name : ens5 method : dhcp force_efi : true device : /dev/vda silent : true iso_url : http://myserver/test.iso poweroff : true no_format : true debug : true tty : ttyS0 vip : 10.10.0.19 vip_hw_addr : 52:54:00:ec:0e:0b vip_mode : dhcp force_mbr : false no_data_partition : false system_settings : auto-disk-provision-paths : \"\"","title":"Configuration Example"},{"location":"install/harvester-configuration/#configuration-reference","text":"Below is a reference of all configuration keys. Warning Security Risks : The configuration file contains credentials which should be kept secret. Please do not make the configuration file publicly accessible.","title":"Configuration Reference"},{"location":"install/harvester-configuration/#server_url","text":"","title":"server_url"},{"location":"install/harvester-configuration/#definition","text":"The URL of the Harvester server to join as an agent. This configuration is mandatory when the installation is in JOIN mode. It tells the Harvester installer where the main server is. Note To ensure a high availability (HA) Harvester cluster, either use the Harvester main server VIP or a domain name in server_url .","title":"Definition"},{"location":"install/harvester-configuration/#example","text":"server_url : https://someserver:8443 install : mode : join","title":"Example"},{"location":"install/harvester-configuration/#token","text":"","title":"token"},{"location":"install/harvester-configuration/#definition_1","text":"The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match what the server has.","title":"Definition"},{"location":"install/harvester-configuration/#example_1","text":"token : myclustersecret Or a node token token : \"K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4\"","title":"Example"},{"location":"install/harvester-configuration/#osssh_authorized_keys","text":"","title":"os.ssh_authorized_keys"},{"location":"install/harvester-configuration/#definition_2","text":"A list of SSH authorized keys that should be added to the default user, rancher . SSH keys can be obtained from GitHub user accounts by using the format github:${USERNAME} . This is done by downloading the keys from https://github.com/${USERNAME}.keys .","title":"Definition"},{"location":"install/harvester-configuration/#example_2","text":"os : ssh_authorized_keys : - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D\" - \"github:ibuildthecloud\"","title":"Example"},{"location":"install/harvester-configuration/#oswrite_files","text":"A list of files to write to disk on boot. The encoding field specifies the content's encoding. Valid encoding values are: \"\" : content data are written in plain text. In this case, the encoding field can be also omitted. b64 , base64 : content data are base64-encoded. gz , gzip : content data are gzip-compressed. gz+base64 , gzip+base64 , gz+b64 , gzip+b64 : content data are gzip-compressed first and then base64-encoded. Example os : write_files : - encoding : b64 content : CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4... owner : root:root path : /etc/connman/main.conf permissions : '0644' - content : | # My new /etc/sysconfig/samba file SMDBOPTIONS=\"-D\" path : /etc/sysconfig/samba - content : !!binary | f0VMRgIBAQAAAAAAAAAAAAIAPgABAAAAwARAAAAAAABAAAAAAAAAAJAVAAAAAA AEAAHgAdAAYAAAAFAAAAQAAAAAAAAABAAEAAAAAAAEAAQAAAAAAAwAEAAAAAAA AAAAAAAAAwAAAAQAAAAAAgAAAAAAAAACQAAAAAAAAAJAAAAAAAAcAAAAAAAAAB ... path : /bin/arch permissions : '0555' - content : | 15 * * * * root ship_logs path : /etc/crontab","title":"os.write_files"},{"location":"install/harvester-configuration/#oshostname","text":"","title":"os.hostname"},{"location":"install/harvester-configuration/#definition_3","text":"Set the system hostname. If the system hostname is supplied via DHCP, then that value will be used here. If this value is not set and one is not supplied via DHCP, then a random hostname will be generated.","title":"Definition"},{"location":"install/harvester-configuration/#example_3","text":"os : hostname : myhostname","title":"Example"},{"location":"install/harvester-configuration/#osmodules","text":"","title":"os.modules"},{"location":"install/harvester-configuration/#definition_4","text":"A list of kernel modules to be loaded on start.","title":"Definition"},{"location":"install/harvester-configuration/#example_4","text":"os : modules : - kvm - nvme","title":"Example"},{"location":"install/harvester-configuration/#ossysctls","text":"","title":"os.sysctls"},{"location":"install/harvester-configuration/#definition_5","text":"Kernel sysctl to set up on start. These are the typical configurations found in /etc/sysctl.conf . Values must be specified as strings.","title":"Definition"},{"location":"install/harvester-configuration/#example_5","text":"os : sysctls : kernel.printk : 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict : \"1\" # force the YAML parser to read as a string","title":"Example"},{"location":"install/harvester-configuration/#osdns_nameservers","text":"","title":"os.dns_nameservers"},{"location":"install/harvester-configuration/#definition_6","text":"Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS.","title":"Definition"},{"location":"install/harvester-configuration/#example_6","text":"os : dns_nameservers : - 8.8.8.8 - 1.1.1.1","title":"Example"},{"location":"install/harvester-configuration/#osntp_servers","text":"","title":"os.ntp_servers"},{"location":"install/harvester-configuration/#definition_7","text":"Fallback ntp servers to use if NTP is not configured elsewhere in the OS.","title":"Definition"},{"location":"install/harvester-configuration/#example_7","text":"os : ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org","title":"Example"},{"location":"install/harvester-configuration/#ospassword","text":"","title":"os.password"},{"location":"install/harvester-configuration/#definition_8","text":"The password for the default user, rancher . By default, there is no password for the rancher user. If you set a password at runtime it will be reset on the next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to change your password on a Linux system and copy the value of the second field from /etc/shadow . You can also encrypt a password using openssl passwd -1 .","title":"Definition"},{"location":"install/harvester-configuration/#example_8","text":"Encrypted: os : password : \"$1$tYtghCfK$QHa51MS6MVAcfUKuOzNKt0\" Or clear text: os : password : supersecure","title":"Example"},{"location":"install/harvester-configuration/#osenvironment","text":"","title":"os.environment"},{"location":"install/harvester-configuration/#definition_9","text":"Environment variables to be set on K3s and other processes like the boot process. Primary use of this field is to set the HTTP proxy.","title":"Definition"},{"location":"install/harvester-configuration/#example_9","text":"os : environment : http_proxy : http://myserver https_proxy : http://myserver Note This example sets the HTTP(S) proxy for foundational OS components . To set up an HTTP(S) proxy for Harvester components such as fetching external images and backup to S3 services, see Settings/http-proxy .","title":"Example"},{"location":"install/harvester-configuration/#oslabels","text":"","title":"os.labels"},{"location":"install/harvester-configuration/#definition_10","text":"Labels to be added to this Node.","title":"Definition"},{"location":"install/harvester-configuration/#installmode","text":"","title":"install.mode"},{"location":"install/harvester-configuration/#definition_11","text":"Harvester installation mode: create : Creating a new Harvester installation. join : Join an existing Harvester installation. Need to specify server_url .","title":"Definition"},{"location":"install/harvester-configuration/#example_10","text":"install : mode : create","title":"Example"},{"location":"install/harvester-configuration/#installnetworks","text":"","title":"install.networks"},{"location":"install/harvester-configuration/#definition_12","text":"Configure network interfaces for the host machine. Each key-value pair represents a network interface. The key name becomes the network name, and the values are configurations for each network. Valid configuration fields are: method : Method to assign an IP to this network. The following are supported: static : Manually assign an IP and gateway. dhcp : Request an IP from the DHCP server. none : Do nothing. Useful when the interface does not need an IP, such as when creating VLAN network NIC in Harvester. ip : Static IP for this network. Required if static method is chosen. subnet_mask : Subnet mask for this network. Required if static method is chosen. gateway : Gateway for this network. Required if static method is chosen. interfaces : An array of interface names. If provided, the installer then combines these NICs into a single logical bonded interface. interfaces.name : The name of the slave interface for the bonded network. bond_options : Options for bonded interfaces. Refer to here for more info. If not provided, the following options would be used: mode: balance-tlb miimon: 100 Note A network called harvester-mgmt is mandatory to establish a valid management network . Note Harvester uses the systemd net naming scheme . Please make sure the interface name is present on the target machine before installation.","title":"Definition"},{"location":"install/harvester-configuration/#example_11","text":"install : mode : create networks : harvester-mgmt : # The management bond name. This is mandatory. interfaces : - name : ens5 method : dhcp bond_options : mode : balance-tlb miimon : 100 harvester-vlan : # The VLAN network bond name. User can then input `harvester-vlan` in the VLAN NIC setting in the GUI. interfaces : - name : ens6 method : none bond_options : mode : balance-tlb miimon : 100 bond0 : interfaces : - name : ens8 method : static ip : 10.10.18.2 subnet_mask : 255.255.255.0 gateway : 192.168.11.1","title":"Example"},{"location":"install/harvester-configuration/#installforce_efi","text":"Force EFI installation even when EFI is not detected. Default: false .","title":"install.force_efi"},{"location":"install/harvester-configuration/#installdevice","text":"The device to install the OS.","title":"install.device"},{"location":"install/harvester-configuration/#installsilent","text":"Reserved.","title":"install.silent"},{"location":"install/harvester-configuration/#installiso_url","text":"ISO to download and install from if booting from kernel/vmlinuz and not ISO.","title":"install.iso_url"},{"location":"install/harvester-configuration/#installpoweroff","text":"Shutdown the machine after installation instead of rebooting","title":"install.poweroff"},{"location":"install/harvester-configuration/#installno_format","text":"Do not partition and format, assume layout exists already.","title":"install.no_format"},{"location":"install/harvester-configuration/#installdebug","text":"Run the installation with additional logging and debugging enabled for the installed system.","title":"install.debug"},{"location":"install/harvester-configuration/#installtty","text":"","title":"install.tty"},{"location":"install/harvester-configuration/#definition_13","text":"The tty device used for the console.","title":"Definition"},{"location":"install/harvester-configuration/#example_12","text":"install : tty : ttyS0,115200n8","title":"Example"},{"location":"install/harvester-configuration/#installvip","text":"","title":"install.vip"},{"location":"install/harvester-configuration/#installvip_mode","text":"","title":"install.vip_mode"},{"location":"install/harvester-configuration/#installvip_hw_addr","text":"","title":"install.vip_hw_addr"},{"location":"install/harvester-configuration/#definition_14","text":"install.vip : The VIP of the Harvester management endpoint. After installation, users can access the Harvester GUI at the URL https://<VIP> . install.vip_mode dhcp : Harvester will send DHCP requests to get the VIP. The install.vip_hw_addr field needs to be provided. static : Harvester uses a static VIP. install.vip_hw_addr : The hardware address corresponding to the VIP. Users must configure their on-premise DHCP server to offer the configured VIP. The field is mandatory when install.vip_mode is dhcp . See Management Address for more information.","title":"Definition"},{"location":"install/harvester-configuration/#example_13","text":"Configure a static VIP. install : vip : 192.168.0.100 vip_mode : static Configure a DHCP VIP. install : vip : 10.10.0.19 vip_mode : dhcp vip_hw_addr : 52:54:00:ec:0e:0b","title":"Example"},{"location":"install/harvester-configuration/#installforce_mbr","text":"","title":"install.force_mbr"},{"location":"install/harvester-configuration/#definition_15","text":"By default, Harvester uses GPT partitioning scheme on both UEFI and BIOS systems. However, if you face compatibility issues, the MBR partitioning scheme can be forced on BIOS systems. Note Harvester creates an additional partition for storing VM data by default. When force using MBR, install.no_data_partition will be forced to true . In other words, no additional partition will be created and VM data will be stored in a partition shared with the OS data.","title":"Definition"},{"location":"install/harvester-configuration/#example_14","text":"install : force_mbr : true","title":"Example"},{"location":"install/harvester-configuration/#installno_data_partition","text":"","title":"install.no_data_partition"},{"location":"install/harvester-configuration/#definition_16","text":"Do not create an additional disk partition for storing VM data. An OS partition will then be used to store VM data. This is useful when you want to use additional disks to store VM data with the auto-disk-provision-paths setting. Default: false . Warning If VM data is stored in the OS partition and you have created too many VMs, there is a high chance of causing OS to malfunction due to the lack of disk space.","title":"Definition"},{"location":"install/harvester-configuration/#example_15","text":"install : no_data_partition : true","title":"Example"},{"location":"install/harvester-configuration/#system_settings","text":"","title":"system_settings"},{"location":"install/harvester-configuration/#definition_17","text":"You can overwrite the default Harvester system settings by configuring system_settings . See the Settings page for additional information and the list of all the options. Note Overwriting system settings only works when Harvester is installed in \"create\" mode. If you install Harvester in \"join\" mode, this setting is ignored. Installing in \"join\" mode will adopt the system settings from the existing Harvester system.","title":"Definition"},{"location":"install/harvester-configuration/#example_16","text":"The example below overwrites http-proxy and ui-source settings. The values must be a string . system_settings : http-proxy : '{\"httpProxy\": \"http://my.proxy\", \"httpsProxy\": \"https://my.proxy\", \"noProxy\": \"some.internal.svc\"}' ui-source : auto","title":"Example"},{"location":"install/iso-install/","text":"ISO Installation \u00b6 To get the Harvester ISO image, download it from the Github releases page. During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note: This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer option. Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device on which the Harvester cluster will be installed Note: By default, Harvester uses GPT partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select MBR . Configure the hostname and select the network interface for the management network. By default, Harvester will create a bonded NIC named harvester-mgmt , and the IP address can either be configured via DHCP or a static assigned a static one. (Optional) Configure the DNS servers. Use commas as a delimiter. Configure the Virtual IP which you can use to access the cluster or join other nodes to the cluster (Note: If your IP address is configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP) . Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default SSH user is rancher . (Optional) Configure the NTP Servers of the node. This defaults to 0.suse.pool.ntp.org . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here. Otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote server URL. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to customize the host with a Harvester configuration file, enter the HTTP URL here. After confirming the installation options, Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete, it will restart the host. After the restart, the Harvester console containing the management URL and status will be displayed. You can Use F12 to switch between the Harvester console and the Shell. The default URL of the web interface is https://your-virtual-ip . You will be prompted to set the password for the default admin user when logging in for the first time.","title":"ISO Installation"},{"location":"install/iso-install/#iso-installation","text":"To get the Harvester ISO image, download it from the Github releases page. During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note: This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer option. Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device on which the Harvester cluster will be installed Note: By default, Harvester uses GPT partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select MBR . Configure the hostname and select the network interface for the management network. By default, Harvester will create a bonded NIC named harvester-mgmt , and the IP address can either be configured via DHCP or a static assigned a static one. (Optional) Configure the DNS servers. Use commas as a delimiter. Configure the Virtual IP which you can use to access the cluster or join other nodes to the cluster (Note: If your IP address is configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP) . Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default SSH user is rancher . (Optional) Configure the NTP Servers of the node. This defaults to 0.suse.pool.ntp.org . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here. Otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote server URL. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to customize the host with a Harvester configuration file, enter the HTTP URL here. After confirming the installation options, Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete, it will restart the host. After the restart, the Harvester console containing the management URL and status will be displayed. You can Use F12 to switch between the Harvester console and the Shell. The default URL of the web interface is https://your-virtual-ip . You will be prompted to set the password for the default admin user when logging in for the first time.","title":"ISO Installation"},{"location":"install/management-address/","text":"Management Address \u00b6 Harvester provides a fixed virtual IP (VIP) as the management address. You can find the management address on the console dashboard after the installation. Note If you selected the IP address to be configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP How to get the VIP MAC address \u00b6 To get the VIP MAC address, you can run the following command on the management node: $ kubectl get svc -n kube-system ingress-expose -ojsonpath = '{.metadata.annotations}' Example of output: { \"kube-vip.io/hwaddr\" : \"02:00:00:09:7f:3f\" , \"kube-vip.io/requestedIP\" : \"10.84.102.31\" } Usages \u00b6 The management address has two usages. Allows the access to the Harvester API/UI via HTTPS protocol. Is the address the other nodes use to join the cluster.","title":"Management Address"},{"location":"install/management-address/#management-address","text":"Harvester provides a fixed virtual IP (VIP) as the management address. You can find the management address on the console dashboard after the installation. Note If you selected the IP address to be configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP","title":"Management Address"},{"location":"install/management-address/#how-to-get-the-vip-mac-address","text":"To get the VIP MAC address, you can run the following command on the management node: $ kubectl get svc -n kube-system ingress-expose -ojsonpath = '{.metadata.annotations}' Example of output: { \"kube-vip.io/hwaddr\" : \"02:00:00:09:7f:3f\" , \"kube-vip.io/requestedIP\" : \"10.84.102.31\" }","title":"How to get the VIP MAC address"},{"location":"install/management-address/#usages","text":"The management address has two usages. Allows the access to the Harvester API/UI via HTTPS protocol. Is the address the other nodes use to join the cluster.","title":"Usages"},{"location":"install/pxe-boot-install/","text":"PXE Boot Installation \u00b6 Starting from version 0.2.0 , Harvester can be installed automatically. This document provides an example to do an automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If the iPXE firmware is not available for your NIC card, the iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit Harvester iPXE Examples . Prerequisite \u00b6 Important Nodes need to have at least 8G of RAM because the installer loads the full ISO file into tmpfs. Preparing HTTP Servers \u00b6 An HTTP server is required to serve boot files. Let's assume the NGINX HTTP server's IP is 10.100.0.10 , and it serves the /usr/share/nginx/html/ directory with the path http://10.100.0.10/ . Preparing Boot Files \u00b6 Download the required files from the Harvester releases page . The ISO: harvester-<version>-amd64.iso The kernel: harvester-<version>-vmlinuz-amd64 The initrd: harvester-<version>-initrd-amd64 The rootfs squashfs image: harvester-<version>-rootfs-amd64.squashfs Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. For example: sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-initrd-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-rootfs-amd64.squashfs /usr/share/nginx/html/harvester/ Preparing iPXE Boot Scripts \u00b6 When performing an automatic installation, there are two modes: CREATE : we are installing a node to construct an initial Harvester cluster. JOIN : we are installing a node to join an existing Harvester cluster. CREATE Mode \u00b6 Warning Security Risks : The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml token : token os : hostname : node1 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin password : rancher install : mode : create networks : harvester-mgmt : # (Mandatory) The management bond name. interfaces : - name : ens5 method : dhcp bond_options : mode : balance-tlb miimon : 100 harvester-vlan : # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces : # host to host, consider creating a bonding device. Users can then select - name : ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method : none bond_options : mode : balance-tlb miimon : 100 device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso vip : 10.100.0.99 # The VIP to access the Harvester GUI. Make sure the IP is free to use. vip_mode : static # Or dhcp, check configuration file for more information. For machines that needs to be installed using CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create . Note If you have multiple network interfaces, you can leverage dracut's ip= parameter to specify the booting interface and any other network configurations that dracut supports (e.g., ip=eth1:dhcp ) See man dracut.cmdline for more information. Use ip= parameter to designate the booting interface only, as we only support one single ip= parameter . JOIN Mode \u00b6 Warning Security Risks : The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml server_url : https://10.100.0.99:8443 # Should be the VIP set up in \"CREATE\" config token : token os : hostname : node2 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin dns_nameservers : - 1.1.1.1 - 8.8.8.8 password : rancher install : mode : join networks : harvester-mgmt : # (Mandatory) The management bond name. interfaces : - name : ens5 method : dhcp bond_options : mode : balance-tlb miimon : 10 harvester-vlan : # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces : # host to host, consider creating a bonding device. Users can then select - name : ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method : none bond_options : mode : balance-tlb miimon : 100 device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd harvester-<version>-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join . DHCP Server Configuration \u00b6 The following is an example of how to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16 ; subnet 10 .100.0.0 netmask 255 .255.255.0 { option routers 10 .100.0.10 ; option domain-name-servers 192 .168.2.1 ; range 10 .100.0.100 10 .100.0.253 ; } group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } group { # join group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-join-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-join\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node2 { hardware ethernet 52 :54:00:69:d5:92 ; } } The config file declares a subnet and two groups. The first group is for hosts to boot using CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client it offers the iPXE image according to the client architecture. Please prepare those images and a TFTP server first. Harvester Configuration \u00b6 For more information about Harvester configuration, please refer to the Harvester configuration page. Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, users can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file. UEFI HTTP Boot support \u00b6 UEFI firmware supports loading a boot image from an HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform an automatic installation. Serve the iPXE Program \u00b6 Download the iPXE UEFI program from http://boot.ipxe.org/ipxe.efi and make sure ipxe.efi can be downloaded from the HTTP server. For example: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi locally. DHCP Server Configuration \u00b6 If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provide the iPXE program URL when it sees such a request. The following is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } elsif substring ( option vendor-class-identifier, 0 , 10 ) = \"HTTPClient\" { # UEFI HTTP Boot option vendor-class-identifier \"HTTPClient\" ; filename \"http://10.100.0.10/harvester/ipxe.efi\" ; } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from the URL http://10.100.0.10/harvester/ipxe-create-efi . The iPXE Script for UEFI Boot \u00b6 It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. The following is an updated version of iPXE script for CREATE mode. #!ipxe kernel harvester-<version>-vmlinuz initrd=harvester-<version>-initrd ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot The parameter initrd=harvester-<version>-initrd is required.","title":"PXE Boot Installation"},{"location":"install/pxe-boot-install/#pxe-boot-installation","text":"Starting from version 0.2.0 , Harvester can be installed automatically. This document provides an example to do an automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If the iPXE firmware is not available for your NIC card, the iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit Harvester iPXE Examples .","title":"PXE Boot Installation"},{"location":"install/pxe-boot-install/#prerequisite","text":"Important Nodes need to have at least 8G of RAM because the installer loads the full ISO file into tmpfs.","title":"Prerequisite"},{"location":"install/pxe-boot-install/#preparing-http-servers","text":"An HTTP server is required to serve boot files. Let's assume the NGINX HTTP server's IP is 10.100.0.10 , and it serves the /usr/share/nginx/html/ directory with the path http://10.100.0.10/ .","title":"Preparing HTTP Servers"},{"location":"install/pxe-boot-install/#preparing-boot-files","text":"Download the required files from the Harvester releases page . The ISO: harvester-<version>-amd64.iso The kernel: harvester-<version>-vmlinuz-amd64 The initrd: harvester-<version>-initrd-amd64 The rootfs squashfs image: harvester-<version>-rootfs-amd64.squashfs Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. For example: sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-initrd-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-rootfs-amd64.squashfs /usr/share/nginx/html/harvester/","title":"Preparing Boot Files"},{"location":"install/pxe-boot-install/#preparing-ipxe-boot-scripts","text":"When performing an automatic installation, there are two modes: CREATE : we are installing a node to construct an initial Harvester cluster. JOIN : we are installing a node to join an existing Harvester cluster.","title":"Preparing iPXE Boot Scripts"},{"location":"install/pxe-boot-install/#create-mode","text":"Warning Security Risks : The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml token : token os : hostname : node1 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin password : rancher install : mode : create networks : harvester-mgmt : # (Mandatory) The management bond name. interfaces : - name : ens5 method : dhcp bond_options : mode : balance-tlb miimon : 100 harvester-vlan : # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces : # host to host, consider creating a bonding device. Users can then select - name : ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method : none bond_options : mode : balance-tlb miimon : 100 device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso vip : 10.100.0.99 # The VIP to access the Harvester GUI. Make sure the IP is free to use. vip_mode : static # Or dhcp, check configuration file for more information. For machines that needs to be installed using CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create . Note If you have multiple network interfaces, you can leverage dracut's ip= parameter to specify the booting interface and any other network configurations that dracut supports (e.g., ip=eth1:dhcp ) See man dracut.cmdline for more information. Use ip= parameter to designate the booting interface only, as we only support one single ip= parameter .","title":"CREATE Mode"},{"location":"install/pxe-boot-install/#join-mode","text":"Warning Security Risks : The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml server_url : https://10.100.0.99:8443 # Should be the VIP set up in \"CREATE\" config token : token os : hostname : node2 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin dns_nameservers : - 1.1.1.1 - 8.8.8.8 password : rancher install : mode : join networks : harvester-mgmt : # (Mandatory) The management bond name. interfaces : - name : ens5 method : dhcp bond_options : mode : balance-tlb miimon : 10 harvester-vlan : # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces : # host to host, consider creating a bonding device. Users can then select - name : ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method : none bond_options : mode : balance-tlb miimon : 100 device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd harvester-<version>-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join .","title":"JOIN Mode"},{"location":"install/pxe-boot-install/#dhcp-server-configuration","text":"The following is an example of how to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16 ; subnet 10 .100.0.0 netmask 255 .255.255.0 { option routers 10 .100.0.10 ; option domain-name-servers 192 .168.2.1 ; range 10 .100.0.100 10 .100.0.253 ; } group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } group { # join group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-join-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-join\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node2 { hardware ethernet 52 :54:00:69:d5:92 ; } } The config file declares a subnet and two groups. The first group is for hosts to boot using CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client it offers the iPXE image according to the client architecture. Please prepare those images and a TFTP server first.","title":"DHCP Server Configuration"},{"location":"install/pxe-boot-install/#harvester-configuration","text":"For more information about Harvester configuration, please refer to the Harvester configuration page. Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, users can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file.","title":"Harvester Configuration"},{"location":"install/pxe-boot-install/#uefi-http-boot-support","text":"UEFI firmware supports loading a boot image from an HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform an automatic installation.","title":"UEFI HTTP Boot support"},{"location":"install/pxe-boot-install/#serve-the-ipxe-program","text":"Download the iPXE UEFI program from http://boot.ipxe.org/ipxe.efi and make sure ipxe.efi can be downloaded from the HTTP server. For example: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi locally.","title":"Serve the iPXE Program"},{"location":"install/pxe-boot-install/#dhcp-server-configuration_1","text":"If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provide the iPXE program URL when it sees such a request. The following is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } elsif substring ( option vendor-class-identifier, 0 , 10 ) = \"HTTPClient\" { # UEFI HTTP Boot option vendor-class-identifier \"HTTPClient\" ; filename \"http://10.100.0.10/harvester/ipxe.efi\" ; } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from the URL http://10.100.0.10/harvester/ipxe-create-efi .","title":"DHCP Server Configuration"},{"location":"install/pxe-boot-install/#the-ipxe-script-for-uefi-boot","text":"It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. The following is an updated version of iPXE script for CREATE mode. #!ipxe kernel harvester-<version>-vmlinuz initrd=harvester-<version>-initrd ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot The parameter initrd=harvester-<version>-initrd is required.","title":"The iPXE Script for UEFI Boot"},{"location":"install/usb-install/","text":"USB Installation \u00b6 Create a bootable USB flash drive \u00b6 There are a couple of ways to create a USB installation flash drive. balenaEtcher \u00b6 balenaEtcher supports writing images to USB flash drives. It has a GUI and is easy to use. Select the Harvester installation ISO and the target USB device to create a USB installation flash drive. dd command \u00b6 On Linux or other platforms that have the dd command, users can use dd to create a USB installation flash drive. Warning Make sure you choose the correct device. The process erases data on the selected device. # sudo dd if=<path_to_iso> of=<path_to_usb_device> bs=64k Common issues \u00b6 When booting from a USB installation flash drive, a GRUB _ text is displayed, but nothing happens \u00b6 If you are using the UEFI mode, try to boot from the UEFI boot partition on the USB device rather than the USB device itself. e.g., Select the UEFI: USB disk 3.0 PMAP, Partition 1 to boot. Note the representation varies from system to system. Graphics issue \u00b6 Firmwares of some graphic cards are not shipped in v0.3.0 . You can press e to edit the GRUB menu entry and append nomodeset to the boot parameters. Press Ctrl + x to boot. Other issues \u00b6 Harvester installer is not displayed If a USB flash driver boots, but you can't see the harvester installer. You may try out the following workarounds: Plug the USB flash drive into a USB 2.0 slot. For version v0.3.0 or above, try to remove the console=ttyS0 parameter when booting. You can press e to edit the GRUB menu entry and remove the console=ttyS0 parameter.","title":"USB Installation"},{"location":"install/usb-install/#usb-installation","text":"","title":"USB Installation"},{"location":"install/usb-install/#create-a-bootable-usb-flash-drive","text":"There are a couple of ways to create a USB installation flash drive.","title":"Create a bootable USB flash drive"},{"location":"install/usb-install/#balenaetcher","text":"balenaEtcher supports writing images to USB flash drives. It has a GUI and is easy to use. Select the Harvester installation ISO and the target USB device to create a USB installation flash drive.","title":"balenaEtcher"},{"location":"install/usb-install/#dd-command","text":"On Linux or other platforms that have the dd command, users can use dd to create a USB installation flash drive. Warning Make sure you choose the correct device. The process erases data on the selected device. # sudo dd if=<path_to_iso> of=<path_to_usb_device> bs=64k","title":"dd command"},{"location":"install/usb-install/#common-issues","text":"","title":"Common issues"},{"location":"install/usb-install/#when-booting-from-a-usb-installation-flash-drive-a-grub-_-text-is-displayed-but-nothing-happens","text":"If you are using the UEFI mode, try to boot from the UEFI boot partition on the USB device rather than the USB device itself. e.g., Select the UEFI: USB disk 3.0 PMAP, Partition 1 to boot. Note the representation varies from system to system.","title":"When booting from a USB installation flash drive, a GRUB _ text is displayed, but nothing happens"},{"location":"install/usb-install/#graphics-issue","text":"Firmwares of some graphic cards are not shipped in v0.3.0 . You can press e to edit the GRUB menu entry and append nomodeset to the boot parameters. Press Ctrl + x to boot.","title":"Graphics issue"},{"location":"install/usb-install/#other-issues","text":"Harvester installer is not displayed If a USB flash driver boots, but you can't see the harvester installer. You may try out the following workarounds: Plug the USB flash drive into a USB 2.0 slot. For version v0.3.0 or above, try to remove the console=ttyS0 parameter when booting. You can press e to edit the GRUB menu entry and remove the console=ttyS0 parameter.","title":"Other issues"},{"location":"monitoring/monitoring/","text":"Monitoring \u00b6 Available as of v0.3.0 Dashboard Metrics \u00b6 Harvester v0.3.0 has provided a built-in monitoring integration using Prometheus . Monitoring is automatically installed during ISO installations. From the Dashboard page, users can view the cluster metrics and top 10 most used VM metrics respectively. Also, users can click the Grafana dashboard link to view more dashboard on the Grafana UI. Note Only admin users are able to view the dashboard metrics. VM Detail Metrics \u00b6 For each VM, users can view the VM metrics by clicking the VM details page.","title":"Monitoring"},{"location":"monitoring/monitoring/#monitoring","text":"Available as of v0.3.0","title":"Monitoring"},{"location":"monitoring/monitoring/#dashboard-metrics","text":"Harvester v0.3.0 has provided a built-in monitoring integration using Prometheus . Monitoring is automatically installed during ISO installations. From the Dashboard page, users can view the cluster metrics and top 10 most used VM metrics respectively. Also, users can click the Grafana dashboard link to view more dashboard on the Grafana UI. Note Only admin users are able to view the dashboard metrics.","title":"Dashboard Metrics"},{"location":"monitoring/monitoring/#vm-detail-metrics","text":"For each VM, users can view the VM metrics by clicking the VM details page.","title":"VM Detail Metrics"},{"location":"networking/harvester-network/","text":"Harvester Network \u00b6 Harvester is built on top of Kubernetes and leverages its built-in CNI mechanism to provide the interface between network providers and its VM networks. We have implemented the Harvester VLAN network based on the bridge CNI to provide a pure L2-mode network, that would bridge your VMs to the host network interface and can be connected using the physical switch for both internal and external network communication. Moreover, the Harvester UI integrates the harvester-network-controller to provide user-friendly VLAN network configurations, e.g., to create and manage VLAN networks or to add a VLAN network to the VM. Currently, Harvester supports two types of networks: Management Network VLAN Network Management Network \u00b6 Harvester uses canal as its default management network. It is a built-in network that can be used directly from the cluster. By default, the management network IP of a VM can only be accessed within the cluster nodes, and the management network IP will not remain unchanged after the VM reboot. However, users can leverage the Kubernetes service object to create a stable IP for your VMs with the management network. VLAN Network \u00b6 The Harvester network-controller leverages the multus and bridge CNI plugins to implement its customized L2 bridge VLAN network. It helps to connect your VMs to the host network interface and can be accessed from internal and external networks using the physical switch. The below diagram illustrates how the VLAN network works in Harvester. The Harvester network-controller creates a bridge for each node and a pair of veth for each VM to implement its VLAN network. The bridge acts as a switch to forward the network traffic from or to VMs and the veth pair is like the connected ports between VMs and the switch. VMs within the same VLAN can communicate with each other, while the VMs from different VLANs can't. The external switch ports connected to the hosts or other devices (such as the DHCP server) should be set as trunk or hybrid type and permit the specified VLANs. Users can use VLAN with PVID (default 1) to communicate with any normal untagged traffic. Enabling Default VLAN Network \u00b6 You can enable VLAN network via Settings > vlan . Select enabled and you will be able to select one network interface from the nodes as the default VLAN NIC config. For better network performances and isolation, we recommend to choose different network interfaces for the VLAN and the one used for the management network (i.e., harvester-mgmt ). Note When selecting the network interface, the value in parentheses represents the distribution percentage of the network interface on all hosts. If a network interface with a value less than 100% is selected, the network interface needs to be manually specified on the host where the VLAN network configuration fails. Modifying the default VLAN network setting will not update the existing configured host network. Harvester VLAN network supports bond interfaces. Currently it can only be created automatically via PEX Boot Configuration . You may also login to the node and create it manually. You can also customize each node's VLAN network via the Hosts > Network tab. Create a VLAN Network \u00b6 A new VLAN network can be created via the Advanced > Networks page and clicking the Create button. Specify the name and VLAN ID that you want to create for the VLAN network (You can specify the same vlan ID on different namespaces of Rancher multi-tenancy support) . Configure a route in order to allow the hosts to connect to the VLAN network using IPv4 addresses. The CIDR and gateway of the VLAN network are mandatory parameters for the route configuration. You can configure the route by choosing one of the following options: auto(DHCP) mode: the Harvester network controller will get the CIDR and gateway values from the DHCP server using the DHCP protocol. Optionally, you can specify the DHCP server address. manual mode: You need to specify the CIDR and gateway values manually. Create a VM with VLAN Network \u00b6 Users can now create a new VM using the above configured VLAN network, Click the Create button on the Virtual Machines page. Specify the required parameters and click the Networks tab. Either configure the default network to be a VLAN network or select an additional network to add. Note Only the first NIC will be enabled by default. Users can either choose to use a management network or a VLAN network. You will need to select the Install guest agent option in the Advanced Options tab to get the VLAN network IP address from the Harvester UI. You can choose to add one or multiple network interface cards. The additional network interface cards can be enabled by default via the cloud-init network data setting. e.g., version : 1 config : - type : physical name : enp1s0 # name is varies upon OS image subnets : - type : dhcp - type : physical name : enp2s0 subnets : - type : DHCP Harvester is fully compatible with the cloud-init network configs . You can refer to the documentation for more details. Configure DHCP servers on Networks \u00b6 By default, the Harvester VLAN network would expect your router to provide a DHCP server that VMs can request and assign IP addresses automatically. If you are running Harvester in a virtual environment that does not contain a DHCP server, you may consider deploying a DHCP server manually on a node or using a containerized method, e.g, like #947 .","title":"Harvester Network"},{"location":"networking/harvester-network/#harvester-network","text":"Harvester is built on top of Kubernetes and leverages its built-in CNI mechanism to provide the interface between network providers and its VM networks. We have implemented the Harvester VLAN network based on the bridge CNI to provide a pure L2-mode network, that would bridge your VMs to the host network interface and can be connected using the physical switch for both internal and external network communication. Moreover, the Harvester UI integrates the harvester-network-controller to provide user-friendly VLAN network configurations, e.g., to create and manage VLAN networks or to add a VLAN network to the VM. Currently, Harvester supports two types of networks: Management Network VLAN Network","title":"Harvester Network"},{"location":"networking/harvester-network/#management-network","text":"Harvester uses canal as its default management network. It is a built-in network that can be used directly from the cluster. By default, the management network IP of a VM can only be accessed within the cluster nodes, and the management network IP will not remain unchanged after the VM reboot. However, users can leverage the Kubernetes service object to create a stable IP for your VMs with the management network.","title":"Management Network"},{"location":"networking/harvester-network/#vlan-network","text":"The Harvester network-controller leverages the multus and bridge CNI plugins to implement its customized L2 bridge VLAN network. It helps to connect your VMs to the host network interface and can be accessed from internal and external networks using the physical switch. The below diagram illustrates how the VLAN network works in Harvester. The Harvester network-controller creates a bridge for each node and a pair of veth for each VM to implement its VLAN network. The bridge acts as a switch to forward the network traffic from or to VMs and the veth pair is like the connected ports between VMs and the switch. VMs within the same VLAN can communicate with each other, while the VMs from different VLANs can't. The external switch ports connected to the hosts or other devices (such as the DHCP server) should be set as trunk or hybrid type and permit the specified VLANs. Users can use VLAN with PVID (default 1) to communicate with any normal untagged traffic.","title":"VLAN Network"},{"location":"networking/harvester-network/#enabling-default-vlan-network","text":"You can enable VLAN network via Settings > vlan . Select enabled and you will be able to select one network interface from the nodes as the default VLAN NIC config. For better network performances and isolation, we recommend to choose different network interfaces for the VLAN and the one used for the management network (i.e., harvester-mgmt ). Note When selecting the network interface, the value in parentheses represents the distribution percentage of the network interface on all hosts. If a network interface with a value less than 100% is selected, the network interface needs to be manually specified on the host where the VLAN network configuration fails. Modifying the default VLAN network setting will not update the existing configured host network. Harvester VLAN network supports bond interfaces. Currently it can only be created automatically via PEX Boot Configuration . You may also login to the node and create it manually. You can also customize each node's VLAN network via the Hosts > Network tab.","title":"Enabling Default VLAN Network"},{"location":"networking/harvester-network/#create-a-vlan-network","text":"A new VLAN network can be created via the Advanced > Networks page and clicking the Create button. Specify the name and VLAN ID that you want to create for the VLAN network (You can specify the same vlan ID on different namespaces of Rancher multi-tenancy support) . Configure a route in order to allow the hosts to connect to the VLAN network using IPv4 addresses. The CIDR and gateway of the VLAN network are mandatory parameters for the route configuration. You can configure the route by choosing one of the following options: auto(DHCP) mode: the Harvester network controller will get the CIDR and gateway values from the DHCP server using the DHCP protocol. Optionally, you can specify the DHCP server address. manual mode: You need to specify the CIDR and gateway values manually.","title":"Create a VLAN Network"},{"location":"networking/harvester-network/#create-a-vm-with-vlan-network","text":"Users can now create a new VM using the above configured VLAN network, Click the Create button on the Virtual Machines page. Specify the required parameters and click the Networks tab. Either configure the default network to be a VLAN network or select an additional network to add. Note Only the first NIC will be enabled by default. Users can either choose to use a management network or a VLAN network. You will need to select the Install guest agent option in the Advanced Options tab to get the VLAN network IP address from the Harvester UI. You can choose to add one or multiple network interface cards. The additional network interface cards can be enabled by default via the cloud-init network data setting. e.g., version : 1 config : - type : physical name : enp1s0 # name is varies upon OS image subnets : - type : dhcp - type : physical name : enp2s0 subnets : - type : DHCP Harvester is fully compatible with the cloud-init network configs . You can refer to the documentation for more details.","title":"Create a VM with VLAN Network"},{"location":"networking/harvester-network/#configure-dhcp-servers-on-networks","text":"By default, the Harvester VLAN network would expect your router to provide a DHCP server that VMs can request and assign IP addresses automatically. If you are running Harvester in a virtual environment that does not contain a DHCP server, you may consider deploying a DHCP server manually on a node or using a containerized method, e.g, like #947 .","title":"Configure DHCP servers on Networks"},{"location":"rancher/cloud-provider/","text":"Harvester Cloud Provider \u00b6 RKE1 and RKE2 clusters can be provisioned in Rancher using the built-in Harvester Node Driver. Harvester provides load balancer and cluster Persistent Storage support to the guest Kubernetes cluster. In this page we will learn: How to deploy the Harvester cloud provider in both RKE1 and RKE2. How to use the Harvester load balancer . Deploying \u00b6 Prerequisites \u00b6 The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace. Deploying to the RKE1 Cluster with Harvester Node Driver \u00b6 When spinning up an RKE cluster using the Harvester node driver, you can perform two steps to deploy the Harvester cloud provider: Select Harvester(Out-of-tree) option. Install Harvester Cloud Provider from the Rancher marketplace. Note You should specify the Cluster name . The default value kubernetes will be set if no Cluster name is entered. The Cluster name is used to distinguish the ownership of the Harvester load balancers. Install Harvester csi driver from the Rancher marketplace if needed. Deploying to the RKE2 Cluster with Harvester Node Driver \u00b6 When spinning up an RKE2 cluster using the Harvester node driver, select the Harvester cloud provider. The node driver will then help deploy both the CSI driver and CCM automatically. Load Balancer Support \u00b6 After deploying the Harvester Cloud provider , you can use the Kubernetes LoadBalancer service to expose a microservice inside the guest cluster to the external world. When you create a Kubernetes LoadBalancer service, a Harvester load balancer is assigned to the service and you can edit it through the Add-on Config in the Rancher UI. IPAM \u00b6 Harvester's built-in load balancer supports both pool and dhcp modes. You can select the mode in the Rancher UI. Harvester adds the annotation cloudprovider.harvesterhci.io/healthcheck-port to the service behind. pool: You should configure an IP address pool in Harvester in advance. The Harvester LoadBalancer controller will allocate an IP address from the IP address pool for the load balancer. dhcp: A DHCP server is required. The Harvester LoadBalancer controller will request an IP address from the DHCP server. Note It is not allowed to modify the IPAM mode. You need to create a new service if you want to modify the IPAM mode. Health Checks \u00b6 The Harvester load balancer supports TCP health checks. You can specify the parameters in the Rancher UI if you enable the Health Check option. Alternatively, you can speficy the parameters by adding annotations to the service manually. The following annotations are supported: Annotation Key Value Type Required Description cloudprovider.harvesterhci.io/healthcheck-port string true Specifies the port. The prober will access the address composed of the backend server IP and the port. cloudprovider.harvesterhci.io/healthcheck-success-threshold string false Specifies the health check success threshold. The default value is 1. The backend server will start forwarding traffic if the number of times the prober continuously detects an address successfully reaches the threshold. cloudprovider.harvesterhci.io/healthcheck-failure-threshold string false Specifies the health check failure threshold. The default value is 3. The backend server will stop forwarding traffic if the number of health check failures reaches the threshold. cloudprovider.harvesterhci.io/healthcheck-periodseconds string false Specifies the health check period. The default value is 5 seconds. cloudprovider.harvesterhci.io/healthcheck-timeoutseconds string false Specifies the timeout of every health check. The default value is 3 seconds. Note Currently, the health check port needs to be a nodeport . There's a feature request to change it to a service port and which will be implemented in a future release.","title":"Harvester Cloud Provider"},{"location":"rancher/cloud-provider/#harvester-cloud-provider","text":"RKE1 and RKE2 clusters can be provisioned in Rancher using the built-in Harvester Node Driver. Harvester provides load balancer and cluster Persistent Storage support to the guest Kubernetes cluster. In this page we will learn: How to deploy the Harvester cloud provider in both RKE1 and RKE2. How to use the Harvester load balancer .","title":"Harvester Cloud Provider"},{"location":"rancher/cloud-provider/#deploying","text":"","title":"Deploying"},{"location":"rancher/cloud-provider/#prerequisites","text":"The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace.","title":"Prerequisites"},{"location":"rancher/cloud-provider/#deploying-to-the-rke1-cluster-with-harvester-node-driver","text":"When spinning up an RKE cluster using the Harvester node driver, you can perform two steps to deploy the Harvester cloud provider: Select Harvester(Out-of-tree) option. Install Harvester Cloud Provider from the Rancher marketplace. Note You should specify the Cluster name . The default value kubernetes will be set if no Cluster name is entered. The Cluster name is used to distinguish the ownership of the Harvester load balancers. Install Harvester csi driver from the Rancher marketplace if needed.","title":"Deploying to the RKE1 Cluster with Harvester Node Driver"},{"location":"rancher/cloud-provider/#deploying-to-the-rke2-cluster-with-harvester-node-driver","text":"When spinning up an RKE2 cluster using the Harvester node driver, select the Harvester cloud provider. The node driver will then help deploy both the CSI driver and CCM automatically.","title":"Deploying to the RKE2 Cluster with Harvester Node Driver"},{"location":"rancher/cloud-provider/#load-balancer-support","text":"After deploying the Harvester Cloud provider , you can use the Kubernetes LoadBalancer service to expose a microservice inside the guest cluster to the external world. When you create a Kubernetes LoadBalancer service, a Harvester load balancer is assigned to the service and you can edit it through the Add-on Config in the Rancher UI.","title":"Load Balancer Support"},{"location":"rancher/cloud-provider/#ipam","text":"Harvester's built-in load balancer supports both pool and dhcp modes. You can select the mode in the Rancher UI. Harvester adds the annotation cloudprovider.harvesterhci.io/healthcheck-port to the service behind. pool: You should configure an IP address pool in Harvester in advance. The Harvester LoadBalancer controller will allocate an IP address from the IP address pool for the load balancer. dhcp: A DHCP server is required. The Harvester LoadBalancer controller will request an IP address from the DHCP server. Note It is not allowed to modify the IPAM mode. You need to create a new service if you want to modify the IPAM mode.","title":"IPAM"},{"location":"rancher/cloud-provider/#health-checks","text":"The Harvester load balancer supports TCP health checks. You can specify the parameters in the Rancher UI if you enable the Health Check option. Alternatively, you can speficy the parameters by adding annotations to the service manually. The following annotations are supported: Annotation Key Value Type Required Description cloudprovider.harvesterhci.io/healthcheck-port string true Specifies the port. The prober will access the address composed of the backend server IP and the port. cloudprovider.harvesterhci.io/healthcheck-success-threshold string false Specifies the health check success threshold. The default value is 1. The backend server will start forwarding traffic if the number of times the prober continuously detects an address successfully reaches the threshold. cloudprovider.harvesterhci.io/healthcheck-failure-threshold string false Specifies the health check failure threshold. The default value is 3. The backend server will stop forwarding traffic if the number of health check failures reaches the threshold. cloudprovider.harvesterhci.io/healthcheck-periodseconds string false Specifies the health check period. The default value is 5 seconds. cloudprovider.harvesterhci.io/healthcheck-timeoutseconds string false Specifies the timeout of every health check. The default value is 3 seconds. Note Currently, the health check port needs to be a nodeport . There's a feature request to change it to a service port and which will be implemented in a future release.","title":"Health Checks"},{"location":"rancher/csi-driver/","text":"Harvester CSI Driver \u00b6 The Harvester Container Storage Interface (CSI) Driver provides a CSI interface used by guest Kubernetes clusters in Harvester. It connects to the host cluster and hot-plugs host volumes to the virtual machines (VMs) to provide native storage performance. Deploying \u00b6 Prerequisites \u00b6 The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace. Deploying with Harvester RKE2 Node Driver \u00b6 When spinning up a Kubernetes cluster using Rancher RKE2 node driver, the Harvester CSI driver will be deployed when Harvester cloud provider is selected. Deploying with Harvester RKE1 Node Driver \u00b6 Select the external cloud provider option. Generate addon configuration and add it in the RKE config YAML. # depend on kubectl to operate the Harvester cluster ./deploy/generate_addon.sh <serviceaccount name> <namespace>","title":"Harvester CSI Driver"},{"location":"rancher/csi-driver/#harvester-csi-driver","text":"The Harvester Container Storage Interface (CSI) Driver provides a CSI interface used by guest Kubernetes clusters in Harvester. It connects to the host cluster and hot-plugs host volumes to the virtual machines (VMs) to provide native storage performance.","title":"Harvester CSI Driver"},{"location":"rancher/csi-driver/#deploying","text":"","title":"Deploying"},{"location":"rancher/csi-driver/#prerequisites","text":"The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace.","title":"Prerequisites"},{"location":"rancher/csi-driver/#deploying-with-harvester-rke2-node-driver","text":"When spinning up a Kubernetes cluster using Rancher RKE2 node driver, the Harvester CSI driver will be deployed when Harvester cloud provider is selected.","title":"Deploying with Harvester RKE2 Node Driver"},{"location":"rancher/csi-driver/#deploying-with-harvester-rke1-node-driver","text":"Select the external cloud provider option. Generate addon configuration and add it in the RKE config YAML. # depend on kubectl to operate the Harvester cluster ./deploy/generate_addon.sh <serviceaccount name> <namespace>","title":"Deploying with Harvester RKE1 Node Driver"},{"location":"rancher/node-driver/","text":"Harvester Node Driver \u00b6 The Harvester node driver is used to provision VMs in the Harvester cluster. In this section, you'll learn how to configure Rancher to use the Harvester node driver to launch and manage Kubernetes clusters. A node driver is the same as a Docker Machine driver , and the project repo is available at harvester/docker-machine-driver-harvester . You can now provision RKE1/RKE2 Kubernetes clusters in Rancher v2.6.3+ with the built-in Harvester node driver. Additionally, Harvester now can provide built-in Load Balancer support as well as raw cluster persistent storage support to the guest Kubernetes cluster. Note Harvester v1.0.0 is compatible with Rancher v2.6.3+ only. Harvester Node Driver \u00b6 The Harvester node driver is enabled by default in Rancher v2.6.3+ . You can go to Cluster Management > Drivers > Node Drivers page to manage the Harvester node driver manually. When the Harvester node driver is enabled, you can create Kubernetes clusters on top of the Harvester cluster and manage them from Rancher. RKE1 Kubernetes Cluster \u00b6 Click to learn how to create RKE1 Kubernetes Clusters . RKE2 Kubernetes Cluster \u00b6 Click to learn how to create RKE2 Kubernetes Clusters .","title":"Harvester Node Driver"},{"location":"rancher/node-driver/#harvester-node-driver","text":"The Harvester node driver is used to provision VMs in the Harvester cluster. In this section, you'll learn how to configure Rancher to use the Harvester node driver to launch and manage Kubernetes clusters. A node driver is the same as a Docker Machine driver , and the project repo is available at harvester/docker-machine-driver-harvester . You can now provision RKE1/RKE2 Kubernetes clusters in Rancher v2.6.3+ with the built-in Harvester node driver. Additionally, Harvester now can provide built-in Load Balancer support as well as raw cluster persistent storage support to the guest Kubernetes cluster. Note Harvester v1.0.0 is compatible with Rancher v2.6.3+ only.","title":"Harvester Node Driver"},{"location":"rancher/node-driver/#harvester-node-driver_1","text":"The Harvester node driver is enabled by default in Rancher v2.6.3+ . You can go to Cluster Management > Drivers > Node Drivers page to manage the Harvester node driver manually. When the Harvester node driver is enabled, you can create Kubernetes clusters on top of the Harvester cluster and manage them from Rancher.","title":"Harvester Node Driver"},{"location":"rancher/node-driver/#rke1-kubernetes-cluster","text":"Click to learn how to create RKE1 Kubernetes Clusters .","title":"RKE1 Kubernetes Cluster"},{"location":"rancher/node-driver/#rke2-kubernetes-cluster","text":"Click to learn how to create RKE2 Kubernetes Clusters .","title":"RKE2 Kubernetes Cluster"},{"location":"rancher/rancher-integration/","text":"Rancher Integration \u00b6 Available as of v0.3.0 Rancher is an open-source multi-cluster management platform. Harvester has integrated Rancher by default starting with Rancher v2.6.1. Note Harvester v1.0.0 is compatible with Rancher v2.6.3 or above only. Users can now import and manage multiple Harvester clusters using the Rancher Virtualization Management page and leverage the Rancher authentication feature and RBAC control for multi-tenancy support. Deploying Rancher \u00b6 To use Rancher with Harvester, please install the Rancher and Harvester in two separated servers. If you want to try out the integration features, you can create a VM in Harvester and install Rancher v2.6.3 or above. Quick Start Guide \u00b6 Begin creation of a custom cluster by provisioning a Linux host. Your host can be any of the following: A cloud-hosted virtual machine (VM) An on-premises VM A bare-metal server Log into your Linux host using your preferred shell, such as PuTTy or a remote terminal connection. From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart = unless-stopped -p 80 :80 -p 443 :443 --privileged rancher/rancher:v2.6.3 Note For more information on how to deploy the Rancher server, please refer to the Rancher documentation . Virtualization Management \u00b6 With Rancher's Virtualization Management feature, you can now import and manage Harvester clusters. By clicking on one of the clusters, you are able to view and manage the imported Harvester cluster resources like Hosts, VMs, images, volumes, etc. Additionally, the Virtualization Management leverages existing Rancher features such as authentication with various auth providers and multi-tenant support. For more details, please check the virtualization management page. Creating Kubernetes Clusters using the Harvester Node Driver \u00b6 Harvester node driver is used to provision VMs in the Harvester cluster, which Rancher uses to launch and manage guest Kubernetes clusters. Starting with Rancher v2.6.1 , the Harvester node driver has been added by default. Users can reference the node-driver page for more details. Note Harvester Node Driver with RKE2/k3s is in Tech Preview.","title":"Rancher integration"},{"location":"rancher/rancher-integration/#rancher-integration","text":"Available as of v0.3.0 Rancher is an open-source multi-cluster management platform. Harvester has integrated Rancher by default starting with Rancher v2.6.1. Note Harvester v1.0.0 is compatible with Rancher v2.6.3 or above only. Users can now import and manage multiple Harvester clusters using the Rancher Virtualization Management page and leverage the Rancher authentication feature and RBAC control for multi-tenancy support.","title":"Rancher Integration"},{"location":"rancher/rancher-integration/#deploying-rancher","text":"To use Rancher with Harvester, please install the Rancher and Harvester in two separated servers. If you want to try out the integration features, you can create a VM in Harvester and install Rancher v2.6.3 or above.","title":"Deploying Rancher"},{"location":"rancher/rancher-integration/#quick-start-guide","text":"Begin creation of a custom cluster by provisioning a Linux host. Your host can be any of the following: A cloud-hosted virtual machine (VM) An on-premises VM A bare-metal server Log into your Linux host using your preferred shell, such as PuTTy or a remote terminal connection. From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart = unless-stopped -p 80 :80 -p 443 :443 --privileged rancher/rancher:v2.6.3 Note For more information on how to deploy the Rancher server, please refer to the Rancher documentation .","title":"Quick Start Guide"},{"location":"rancher/rancher-integration/#virtualization-management","text":"With Rancher's Virtualization Management feature, you can now import and manage Harvester clusters. By clicking on one of the clusters, you are able to view and manage the imported Harvester cluster resources like Hosts, VMs, images, volumes, etc. Additionally, the Virtualization Management leverages existing Rancher features such as authentication with various auth providers and multi-tenant support. For more details, please check the virtualization management page.","title":"Virtualization Management"},{"location":"rancher/rancher-integration/#creating-kubernetes-clusters-using-the-harvester-node-driver","text":"Harvester node driver is used to provision VMs in the Harvester cluster, which Rancher uses to launch and manage guest Kubernetes clusters. Starting with Rancher v2.6.1 , the Harvester node driver has been added by default. Users can reference the node-driver page for more details. Note Harvester Node Driver with RKE2/k3s is in Tech Preview.","title":"Creating Kubernetes Clusters using the Harvester Node Driver"},{"location":"rancher/rke1-cluster/","text":"Creating an RKE1 Kubernetes Cluster \u00b6 You can now provision RKE1 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.3+ with the built-in Harvester node driver. Note VLAN network is required for Harvester node driver. When you create a Kubernetes cluster hosted by the Harvester infrastructure, node templates are used to provision the cluster nodes. These templates use Docker Machine configuration options to define an operating system image and settings/parameters for the node. Node templates can use cloud credentials to access the credentials information required to provision nodes in the infrastructure providers. The same cloud credentials can be used by multiple node templates. By using cloud credentials , you do not have to re-enter access keys for the same cloud provider. Cloud credentials are stored as Kubernetes secrets. You can create cloud credentials in two contexts: During the creation of a node template for a cluster. In the User Settings page All cloud credentials are bound to your user profile and cannot be shared with other users. Create Your Cloud Credentials \u00b6 Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name. Select \"Imported Harvester\" or \"External Harvester\". Click Create . Create Node Template \u00b6 You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure the Cloud Credentials . Configure Instance Options : Configure the CPU, memory, and disk Select an OS image that is compatible with the cloud-init config. Select a network that the node driver is able to connect to; currently, only VLAN is supported. Enter the SSH User; the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu . (Optional) Configure Advanced Options if you want to customise the cloud-init config of the VMs: Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more information. Create RKE1 Kubernetes Cluster \u00b6 Users can create an RKE1 Kubernetes cluster from the Cluster Management page via the Harvester RKE1 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE1 . Select Harvester node driver. Enter Cluster Name (required). Enter Name Prefix (required). Enter Template (required). Select etcd and Control Plane (required). On the Cluster Options configure Cloud Provider to Harvester if you want to use the Harvester Cloud Provider and CSI Diver . Click Create . Using Harvester RKE1 Node Driver in Air Gapped Environment \u00b6 RKE1 provisioning relies on the qemu-guest-agent to get the IP of the virtual machine, and docker to set up the RKE cluster. However, It may not be feasible to install qemu-guest-agent and docker in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with qemu-guest-agent and docker installed. Option 2. Configure the cloud init user data to enable the VMs to install qemu-guest-agent and docker via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 write_files: - path: /etc/environment content: | HTTP_PROXY=\"http://192.168.0.1:3128\" HTTPS_PROXY=\"http://192.168.0.1:3128\" append: true","title":"Creating an RKE1 Kubernetes Cluster"},{"location":"rancher/rke1-cluster/#creating-an-rke1-kubernetes-cluster","text":"You can now provision RKE1 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.3+ with the built-in Harvester node driver. Note VLAN network is required for Harvester node driver. When you create a Kubernetes cluster hosted by the Harvester infrastructure, node templates are used to provision the cluster nodes. These templates use Docker Machine configuration options to define an operating system image and settings/parameters for the node. Node templates can use cloud credentials to access the credentials information required to provision nodes in the infrastructure providers. The same cloud credentials can be used by multiple node templates. By using cloud credentials , you do not have to re-enter access keys for the same cloud provider. Cloud credentials are stored as Kubernetes secrets. You can create cloud credentials in two contexts: During the creation of a node template for a cluster. In the User Settings page All cloud credentials are bound to your user profile and cannot be shared with other users.","title":"Creating an RKE1 Kubernetes Cluster"},{"location":"rancher/rke1-cluster/#create-your-cloud-credentials","text":"Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name. Select \"Imported Harvester\" or \"External Harvester\". Click Create .","title":"Create Your Cloud Credentials"},{"location":"rancher/rke1-cluster/#create-node-template","text":"You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure the Cloud Credentials . Configure Instance Options : Configure the CPU, memory, and disk Select an OS image that is compatible with the cloud-init config. Select a network that the node driver is able to connect to; currently, only VLAN is supported. Enter the SSH User; the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu . (Optional) Configure Advanced Options if you want to customise the cloud-init config of the VMs: Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more information.","title":"Create Node Template"},{"location":"rancher/rke1-cluster/#create-rke1-kubernetes-cluster","text":"Users can create an RKE1 Kubernetes cluster from the Cluster Management page via the Harvester RKE1 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE1 . Select Harvester node driver. Enter Cluster Name (required). Enter Name Prefix (required). Enter Template (required). Select etcd and Control Plane (required). On the Cluster Options configure Cloud Provider to Harvester if you want to use the Harvester Cloud Provider and CSI Diver . Click Create .","title":"Create RKE1 Kubernetes Cluster"},{"location":"rancher/rke1-cluster/#using-harvester-rke1-node-driver-in-air-gapped-environment","text":"RKE1 provisioning relies on the qemu-guest-agent to get the IP of the virtual machine, and docker to set up the RKE cluster. However, It may not be feasible to install qemu-guest-agent and docker in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with qemu-guest-agent and docker installed. Option 2. Configure the cloud init user data to enable the VMs to install qemu-guest-agent and docker via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 write_files: - path: /etc/environment content: | HTTP_PROXY=\"http://192.168.0.1:3128\" HTTPS_PROXY=\"http://192.168.0.1:3128\" append: true","title":"Using Harvester RKE1 Node Driver in Air Gapped Environment"},{"location":"rancher/rke2-cluster/","text":"Creating an RKE2 Kubernetes Cluster \u00b6 Users can now provision RKE2 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1+ using the built-in Harvester node driver. Note Harvester RKE2 node driver is in tech preview. VLAN network is required for Harvester node driver. Create Your Cloud Credentials \u00b6 Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name Select \"Imported Harvester\" or \"External Harvester\". Click Create . Create RKE2 Kubernetes Cluster \u00b6 Users can create a RKE2 Kubernetes cluster from the Cluster Management page via the RKE2 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE2/K3s . Select Harvester node driver. Select a Cloud Credential . Enter Cluster Name (required). Enter Namespace (required). Enter Image (required). Enter Network Name (required). Enter SSH User (required). Click Create . Note RKE2 v1.21.5+rke2r2 or above provides a built-in Harvester Cloud Provider and Guest CSI driver integration. Currently only imported Harvester clusters are supported automatically. Using Harvester RKE2 Node Driver in Air Gapped Environment \u00b6 RKE2 provisioning relies on the qemu-guest-agent to get the IP of the virtual machine. However, it may not be feasible to install qemu-guest-agent in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with qemu-guest-agent installed. Option 2. Configure the cloud init user data to enable the VMs to install qemu-guest-agent via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128","title":"Creating an RKE2 Kubernetes Cluster"},{"location":"rancher/rke2-cluster/#creating-an-rke2-kubernetes-cluster","text":"Users can now provision RKE2 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1+ using the built-in Harvester node driver. Note Harvester RKE2 node driver is in tech preview. VLAN network is required for Harvester node driver.","title":"Creating an RKE2 Kubernetes Cluster"},{"location":"rancher/rke2-cluster/#create-your-cloud-credentials","text":"Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name Select \"Imported Harvester\" or \"External Harvester\". Click Create .","title":"Create Your Cloud Credentials"},{"location":"rancher/rke2-cluster/#create-rke2-kubernetes-cluster","text":"Users can create a RKE2 Kubernetes cluster from the Cluster Management page via the RKE2 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE2/K3s . Select Harvester node driver. Select a Cloud Credential . Enter Cluster Name (required). Enter Namespace (required). Enter Image (required). Enter Network Name (required). Enter SSH User (required). Click Create . Note RKE2 v1.21.5+rke2r2 or above provides a built-in Harvester Cloud Provider and Guest CSI driver integration. Currently only imported Harvester clusters are supported automatically.","title":"Create RKE2 Kubernetes Cluster"},{"location":"rancher/rke2-cluster/#using-harvester-rke2-node-driver-in-air-gapped-environment","text":"RKE2 provisioning relies on the qemu-guest-agent to get the IP of the virtual machine. However, it may not be feasible to install qemu-guest-agent in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with qemu-guest-agent installed. Option 2. Configure the cloud init user data to enable the VMs to install qemu-guest-agent via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128","title":"Using Harvester RKE2 Node Driver in Air Gapped Environment"},{"location":"rancher/virtualization-management/","text":"Virtualization Management \u00b6 For Harvester v0.3.0 and above, virtualization management with the multi-cluster management feature will be supported using Rancher v2.6.x. As a prerequisite, you will need to create a Rancher server with v2.6.3 or above for Harvester v1.0.0 integration. For testing purposes, you can spin up a Rancher server using the following docker run command: $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Note For a production Rancher server setup, please refer to the official Rancher docs . Once the Rancher server is up and running, log in and click the hamburger menu and choose the Virtualization Management tab. Select Import Existing to import the downstream Harvester cluster into the Rancher server. Specify the Cluster Name and click Create . You will then see the registration guide; please open the dashboard of the target Harvester cluster and follow the guide accordingly. Once the agent node is ready, you should be able to view and access the imported Harvester cluster from the Rancher server and manage your VMs accordingly. From the Harvester UI, you can click the hamburger menu to navigate back to the Rancher multi-cluster management page. Multi-Tenancy \u00b6 In Harvester, we have leveraged the existing Rancher RBAC authorization such that users can view and manage a set of resources based on their cluster and project role permissions. Within Rancher, each person authenticates as a user, which is a login that grants a user access to Rancher. As mentioned in Authentication , users can either be local or external. Once the user logs into Rancher, their authorization, also known as access rights, is determined by global permissions and cluster and project roles. Global Permissions : Define user authorization outside the scope of any particular cluster. Cluster and Project Roles : Define user authorization inside the specific cluster or project where users are assigned the role. Both global permissions and cluster and project roles are implemented on top of Kubernetes RBAC . Therefore, enforcement of permissions and roles is performed by Kubernetes. A cluster owner has full control over the cluster and all resources inside it, e.g., hosts, VMs, volumes, images, networks, backups, and settings. A project user can be assigned to a specific project with permission to manage the resources inside the project. Multi-Tenancy Example \u00b6 The following example provides a good explanation of how the multi-tenant feature works: First, add new users via the Rancher Users & Authentication page. Then click Create to add two new separated users, such as project-owner and project-readonly respectively. A project-owner is a user with permission to manage a list of resources of a particular project, e.g., the default project. A project-readonly is a user with read-only permission of a particular project, e.g., the default project. Click one of the imported Harvester clusters after navigating to the Harvester UI. Click the Projects/Namespaces tab. Select a project such as default and click the Edit Config menu to assign the users to this project with appropriate permissions. For example, the project-owner user will be assigned the project owner role. Continue to add the project-readonly user to the same project with read-only permissions and click Save . Open an incognito browser and log in as project-owner . After logging in as the project-owner user, click the Virtualization Management tab. There you should be able to view the cluster to which you have been assigned. Click the Images tab to view a list of images previously uploaded to the harvester-public namespace. You can also upload your own image if needed. Create a VM with one of the images that you have uploaded. Log in with another user, e.g., project-readonly , and this user will only have the read permission of this project. Delete Imported Harvester Cluster \u00b6 Users can delete the imported Harvester cluster from the Rancher UI via Virtualization Management > Harvester Clusters . Select the cluster you want to remove and click the Delete button to delete the imported Harvester cluster. Warning Please do not run the kubectl delete -f ... command to delete the imported Harvester cluster as it will remove the entire cattle-system namespace which is required of the Harvester cluster.","title":"Virtualization management"},{"location":"rancher/virtualization-management/#virtualization-management","text":"For Harvester v0.3.0 and above, virtualization management with the multi-cluster management feature will be supported using Rancher v2.6.x. As a prerequisite, you will need to create a Rancher server with v2.6.3 or above for Harvester v1.0.0 integration. For testing purposes, you can spin up a Rancher server using the following docker run command: $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.3 Note For a production Rancher server setup, please refer to the official Rancher docs . Once the Rancher server is up and running, log in and click the hamburger menu and choose the Virtualization Management tab. Select Import Existing to import the downstream Harvester cluster into the Rancher server. Specify the Cluster Name and click Create . You will then see the registration guide; please open the dashboard of the target Harvester cluster and follow the guide accordingly. Once the agent node is ready, you should be able to view and access the imported Harvester cluster from the Rancher server and manage your VMs accordingly. From the Harvester UI, you can click the hamburger menu to navigate back to the Rancher multi-cluster management page.","title":"Virtualization Management"},{"location":"rancher/virtualization-management/#multi-tenancy","text":"In Harvester, we have leveraged the existing Rancher RBAC authorization such that users can view and manage a set of resources based on their cluster and project role permissions. Within Rancher, each person authenticates as a user, which is a login that grants a user access to Rancher. As mentioned in Authentication , users can either be local or external. Once the user logs into Rancher, their authorization, also known as access rights, is determined by global permissions and cluster and project roles. Global Permissions : Define user authorization outside the scope of any particular cluster. Cluster and Project Roles : Define user authorization inside the specific cluster or project where users are assigned the role. Both global permissions and cluster and project roles are implemented on top of Kubernetes RBAC . Therefore, enforcement of permissions and roles is performed by Kubernetes. A cluster owner has full control over the cluster and all resources inside it, e.g., hosts, VMs, volumes, images, networks, backups, and settings. A project user can be assigned to a specific project with permission to manage the resources inside the project.","title":"Multi-Tenancy"},{"location":"rancher/virtualization-management/#multi-tenancy-example","text":"The following example provides a good explanation of how the multi-tenant feature works: First, add new users via the Rancher Users & Authentication page. Then click Create to add two new separated users, such as project-owner and project-readonly respectively. A project-owner is a user with permission to manage a list of resources of a particular project, e.g., the default project. A project-readonly is a user with read-only permission of a particular project, e.g., the default project. Click one of the imported Harvester clusters after navigating to the Harvester UI. Click the Projects/Namespaces tab. Select a project such as default and click the Edit Config menu to assign the users to this project with appropriate permissions. For example, the project-owner user will be assigned the project owner role. Continue to add the project-readonly user to the same project with read-only permissions and click Save . Open an incognito browser and log in as project-owner . After logging in as the project-owner user, click the Virtualization Management tab. There you should be able to view the cluster to which you have been assigned. Click the Images tab to view a list of images previously uploaded to the harvester-public namespace. You can also upload your own image if needed. Create a VM with one of the images that you have uploaded. Log in with another user, e.g., project-readonly , and this user will only have the read permission of this project.","title":"Multi-Tenancy Example"},{"location":"rancher/virtualization-management/#delete-imported-harvester-cluster","text":"Users can delete the imported Harvester cluster from the Rancher UI via Virtualization Management > Harvester Clusters . Select the cluster you want to remove and click the Delete button to delete the imported Harvester cluster. Warning Please do not run the kubectl delete -f ... command to delete the imported Harvester cluster as it will remove the entire cattle-system namespace which is required of the Harvester cluster.","title":"Delete Imported Harvester Cluster"},{"location":"reference/api/","text":"API Reference \u00b6 const ui = SwaggerUIBundle({ url: 'swagger.json', dom_id: '#swagger-ui', })","title":"API Reference"},{"location":"reference/api/#api-reference","text":"const ui = SwaggerUIBundle({ url: 'swagger.json', dom_id: '#swagger-ui', })","title":"API Reference"},{"location":"settings/settings/","text":"Settings \u00b6 This page contains a list of advanced settings which can be used in Harvester. You can modify the custom resource settings.harvesterhci.io from the Dashboard UI or with the kubectl command. additional-ca \u00b6 This setting allows you to configure additional trusted CA certificates for Harvester to access external services. Default: none Example \u00b6 -----BEGIN CERTIFICATE----- SOME-CA-CERTIFICATES -----END CERTIFICATE----- backup-target \u00b6 This setting allows you to set a custom backup target to store VM backups. It supports NFS and S3. For further information, please refer to the Longhorn documentation . Default: none Example \u00b6 { \"type\" : \"s3\" , \"endpoint\" : \"https://s3.endpoint.svc\" , \"accessKeyId\" : \"test-access-key-id\" , \"secretAccessKey\" : \"test-access-key\" , \"bucketName\" : \"test-bup\" , \"bucketRegion\" : \"us\u2011east\u20112\" , \"cert\" : \"\" , \"virtualHostedStyle\" : false } cluster-registration-url \u00b6 This setting allows you to import the Harvester cluster to Rancher for multi-cluster management. Default: none Example \u00b6 https://172.16.0.1/v3/import/w6tp7dgwjj549l88pr7xmxb4x6m54v5kcplvhbp9vv2wzqrrjhrc7c_c-m-zxbbbck9.yaml http-proxy \u00b6 This setting allows you to configure an HTTP proxy to access external services, including the download of images and backup to s3 services. Default: {} The following options and values can be set: Proxy URL for HTTP requests: \"httpProxy\": \"http://<username>:<pswd>@<ip>:<port>\" Proxy URL for HTTPS requests: \"httpsProxy\": \"https://<username>:<pswd>@<ip>:<port>\" Comma-separated list of hostnames and/or CIDRs: \"noProxy\": \"<hostname | CIDR>\" Example \u00b6 { \"httpProxy\" : \"http://my.proxy\" , \"httpsProxy\" : \"https://my.proxy\" , \"noProxy\" : \"some.internal.svc,172.16.0.0/16\" } Note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,cattle-system.svc,.svc,.cluster.local log-level \u00b6 This setting allows you to configure the log level for the Harvester server. Default: info The following values can be set. The list goes from the least to most verbose log level: panic fatal error warn , warning info debug trace Example \u00b6 debug overcommit-config \u00b6 This setting allows you to configure the percentage for resources overcommit on CPU, memory, and storage. By setting resources overcommit, this will permit to schedule additional virtual machines even if the the physical resources are already fully utilized. Default: { \"cpu\":1600, \"memory\":150, \"storage\":200 } The default CPU overcommit with 1600% means, for example, if the CPU resources limit of a virtual machine is 1600m core, Harvester would only request 100m CPU for it from Kubernetes scheduler. Example \u00b6 { \"cpu\" : 1000 , \"memory\" : 200 , \"storage\" : 300 } server-version \u00b6 This setting displays the version of Harvester server. Example \u00b6 v1.0.0-abcdef-head ssl-certificates \u00b6 This setting allows you to configure serving certificates for Harvester UI/API. Default: {} Example \u00b6 { \"ca\" : \"-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----\" , \"publicCertificate\" : \"-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----\" , \"privateKey\" : \"-----BEGIN RSA PRIVATE KEY-----\\nSOME-PRIVATE-KEY-ENCODED-IN-PEM-FORMAT\\n-----END RSA PRIVATE KEY-----\" } ssl-parameters \u00b6 This setting allows you to configure SSL parameters. Protocols are separated by spaces and ciphers are separated by colons. Default: {} Example \u00b6 { \"protocols\" : \"TLSv1.2 TLSv1.3\" , \"ciphers\" : \"ECDHE-RSA-CHACHA20-POLY1305:ECDHE-RSA-AES256-GCM-SHA384\" } ui-index \u00b6 This setting allows you to configure HTML index location for the UI. Default: https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Example \u00b6 https://your.static.dashboard-ui/index.html ui-source \u00b6 This setting allows you to configure how to load the UI source. The following values can be set: auto : The default. Auto-detect whether to use bundled UI or not. external : Use external UI source. bundled : Use the bundled UI source. Example \u00b6 external upgrade-checker-enabled \u00b6 This setting allows you to automatically check if there's an upgrade available for Harvester. Default: true Example \u00b6 false upgrade-checker-url \u00b6 This setting allows you to configure the URL for the upgrade check of Harvester. Can only be used if the upgrade-checker-enabled setting is set to true. Default: https://harvester-upgrade-responder.rancher.io/v1/checkupgrade Example \u00b6 https://your.upgrade.checker-url/v99/checkupgrade vlan \u00b6 This setting allows you to configure the default physical NIC name of the VLAN network. Default: none Example \u00b6 ens3 auto-disk-provision-paths [Experimental] \u00b6 This setting allows Harvester to automatically add disks that match the given glob pattern as VM storage. It's possible to provide multiple patterns by separating them with a comma. Warning This setting is applied to every Node in the cluster. All the data in these storage devices will be destroyed . Use at your own risk. Default: none Example \u00b6 The following example will add disks matching the glob pattern /dev/sd* or /dev/hd* : /dev/sd*,/dev/hd* ssl-parameters \u00b6 This setting allows you to change the enabled SSL/TLS protocols and ciphers of Harvester GUI and API. The following options and values can be set: protocols : Enabled protocols. See NGINX Ingress Controller's configs ssl-protocols for supported input. ciphers : Enabled ciphers. See NGINX Ingress Controller's configs ssl-ciphers for supported input. If no value is provided, protocols is set to TLSv1.2 only and the ciphers list is ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305 . Default: none Note See Troubleshooting if you have misconfigured this setting and no longer have access to Harvester GUI and API. Example \u00b6 The following example sets the enabled SSL/TLS protocols to TLSv1.2 and TLSv1.3 and the ciphers list to ECDHE-ECDSA-AES128-GCM-SHA256 and ECDHE-ECDSA-CHACHA20-POLY1305 . { \"protocols\": \"TLSv1.2 TLSv1.3\", \"ciphers\": \"ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-CHACHA20-POLY1305\" } vm-force-reset-policy \u00b6 This setting allows you to force reschedule VMs when a node is unavailable. When a node turns to be Not Ready , it will force delete the VM on that node and reschedule it to another available node after a period of seconds. Default: {\"enable\":true, \"period\":300} Example \u00b6 { \"enable\" : \"true\" , \"period\" : 300 }","title":"Settings"},{"location":"settings/settings/#settings","text":"This page contains a list of advanced settings which can be used in Harvester. You can modify the custom resource settings.harvesterhci.io from the Dashboard UI or with the kubectl command.","title":"Settings"},{"location":"settings/settings/#additional-ca","text":"This setting allows you to configure additional trusted CA certificates for Harvester to access external services. Default: none","title":"additional-ca"},{"location":"settings/settings/#example","text":"-----BEGIN CERTIFICATE----- SOME-CA-CERTIFICATES -----END CERTIFICATE-----","title":"Example"},{"location":"settings/settings/#backup-target","text":"This setting allows you to set a custom backup target to store VM backups. It supports NFS and S3. For further information, please refer to the Longhorn documentation . Default: none","title":"backup-target"},{"location":"settings/settings/#example_1","text":"{ \"type\" : \"s3\" , \"endpoint\" : \"https://s3.endpoint.svc\" , \"accessKeyId\" : \"test-access-key-id\" , \"secretAccessKey\" : \"test-access-key\" , \"bucketName\" : \"test-bup\" , \"bucketRegion\" : \"us\u2011east\u20112\" , \"cert\" : \"\" , \"virtualHostedStyle\" : false }","title":"Example"},{"location":"settings/settings/#cluster-registration-url","text":"This setting allows you to import the Harvester cluster to Rancher for multi-cluster management. Default: none","title":"cluster-registration-url"},{"location":"settings/settings/#example_2","text":"https://172.16.0.1/v3/import/w6tp7dgwjj549l88pr7xmxb4x6m54v5kcplvhbp9vv2wzqrrjhrc7c_c-m-zxbbbck9.yaml","title":"Example"},{"location":"settings/settings/#http-proxy","text":"This setting allows you to configure an HTTP proxy to access external services, including the download of images and backup to s3 services. Default: {} The following options and values can be set: Proxy URL for HTTP requests: \"httpProxy\": \"http://<username>:<pswd>@<ip>:<port>\" Proxy URL for HTTPS requests: \"httpsProxy\": \"https://<username>:<pswd>@<ip>:<port>\" Comma-separated list of hostnames and/or CIDRs: \"noProxy\": \"<hostname | CIDR>\"","title":"http-proxy"},{"location":"settings/settings/#example_3","text":"{ \"httpProxy\" : \"http://my.proxy\" , \"httpsProxy\" : \"https://my.proxy\" , \"noProxy\" : \"some.internal.svc,172.16.0.0/16\" } Note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,cattle-system.svc,.svc,.cluster.local","title":"Example"},{"location":"settings/settings/#log-level","text":"This setting allows you to configure the log level for the Harvester server. Default: info The following values can be set. The list goes from the least to most verbose log level: panic fatal error warn , warning info debug trace","title":"log-level"},{"location":"settings/settings/#example_4","text":"debug","title":"Example"},{"location":"settings/settings/#overcommit-config","text":"This setting allows you to configure the percentage for resources overcommit on CPU, memory, and storage. By setting resources overcommit, this will permit to schedule additional virtual machines even if the the physical resources are already fully utilized. Default: { \"cpu\":1600, \"memory\":150, \"storage\":200 } The default CPU overcommit with 1600% means, for example, if the CPU resources limit of a virtual machine is 1600m core, Harvester would only request 100m CPU for it from Kubernetes scheduler.","title":"overcommit-config"},{"location":"settings/settings/#example_5","text":"{ \"cpu\" : 1000 , \"memory\" : 200 , \"storage\" : 300 }","title":"Example"},{"location":"settings/settings/#server-version","text":"This setting displays the version of Harvester server.","title":"server-version"},{"location":"settings/settings/#example_6","text":"v1.0.0-abcdef-head","title":"Example"},{"location":"settings/settings/#ssl-certificates","text":"This setting allows you to configure serving certificates for Harvester UI/API. Default: {}","title":"ssl-certificates"},{"location":"settings/settings/#example_7","text":"{ \"ca\" : \"-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----\" , \"publicCertificate\" : \"-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----\" , \"privateKey\" : \"-----BEGIN RSA PRIVATE KEY-----\\nSOME-PRIVATE-KEY-ENCODED-IN-PEM-FORMAT\\n-----END RSA PRIVATE KEY-----\" }","title":"Example"},{"location":"settings/settings/#ssl-parameters","text":"This setting allows you to configure SSL parameters. Protocols are separated by spaces and ciphers are separated by colons. Default: {}","title":"ssl-parameters"},{"location":"settings/settings/#example_8","text":"{ \"protocols\" : \"TLSv1.2 TLSv1.3\" , \"ciphers\" : \"ECDHE-RSA-CHACHA20-POLY1305:ECDHE-RSA-AES256-GCM-SHA384\" }","title":"Example"},{"location":"settings/settings/#ui-index","text":"This setting allows you to configure HTML index location for the UI. Default: https://releases.rancher.com/harvester-ui/dashboard/latest/index.html","title":"ui-index"},{"location":"settings/settings/#example_9","text":"https://your.static.dashboard-ui/index.html","title":"Example"},{"location":"settings/settings/#ui-source","text":"This setting allows you to configure how to load the UI source. The following values can be set: auto : The default. Auto-detect whether to use bundled UI or not. external : Use external UI source. bundled : Use the bundled UI source.","title":"ui-source"},{"location":"settings/settings/#example_10","text":"external","title":"Example"},{"location":"settings/settings/#upgrade-checker-enabled","text":"This setting allows you to automatically check if there's an upgrade available for Harvester. Default: true","title":"upgrade-checker-enabled"},{"location":"settings/settings/#example_11","text":"false","title":"Example"},{"location":"settings/settings/#upgrade-checker-url","text":"This setting allows you to configure the URL for the upgrade check of Harvester. Can only be used if the upgrade-checker-enabled setting is set to true. Default: https://harvester-upgrade-responder.rancher.io/v1/checkupgrade","title":"upgrade-checker-url"},{"location":"settings/settings/#example_12","text":"https://your.upgrade.checker-url/v99/checkupgrade","title":"Example"},{"location":"settings/settings/#vlan","text":"This setting allows you to configure the default physical NIC name of the VLAN network. Default: none","title":"vlan"},{"location":"settings/settings/#example_13","text":"ens3","title":"Example"},{"location":"settings/settings/#auto-disk-provision-paths-experimental","text":"This setting allows Harvester to automatically add disks that match the given glob pattern as VM storage. It's possible to provide multiple patterns by separating them with a comma. Warning This setting is applied to every Node in the cluster. All the data in these storage devices will be destroyed . Use at your own risk. Default: none","title":"auto-disk-provision-paths [Experimental]"},{"location":"settings/settings/#example_14","text":"The following example will add disks matching the glob pattern /dev/sd* or /dev/hd* : /dev/sd*,/dev/hd*","title":"Example"},{"location":"settings/settings/#ssl-parameters_1","text":"This setting allows you to change the enabled SSL/TLS protocols and ciphers of Harvester GUI and API. The following options and values can be set: protocols : Enabled protocols. See NGINX Ingress Controller's configs ssl-protocols for supported input. ciphers : Enabled ciphers. See NGINX Ingress Controller's configs ssl-ciphers for supported input. If no value is provided, protocols is set to TLSv1.2 only and the ciphers list is ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305 . Default: none Note See Troubleshooting if you have misconfigured this setting and no longer have access to Harvester GUI and API.","title":"ssl-parameters"},{"location":"settings/settings/#example_15","text":"The following example sets the enabled SSL/TLS protocols to TLSv1.2 and TLSv1.3 and the ciphers list to ECDHE-ECDSA-AES128-GCM-SHA256 and ECDHE-ECDSA-CHACHA20-POLY1305 . { \"protocols\": \"TLSv1.2 TLSv1.3\", \"ciphers\": \"ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-CHACHA20-POLY1305\" }","title":"Example"},{"location":"settings/settings/#vm-force-reset-policy","text":"This setting allows you to force reschedule VMs when a node is unavailable. When a node turns to be Not Ready , it will force delete the VM on that node and reschedule it to another available node after a period of seconds. Default: {\"enable\":true, \"period\":300}","title":"vm-force-reset-policy"},{"location":"settings/settings/#example_16","text":"{ \"enable\" : \"true\" , \"period\" : 300 }","title":"Example"},{"location":"terraform/terraform/","text":"Harvester Terraform Provider \u00b6 Requirements \u00b6 Terraform >= 0.13.x Go 1.16 to build the provider plugin Install the Provider \u00b6 Option 1: Download and install the provider from the Terraform registry . \u00b6 To install this provider, copy and paste this code into your Terraform configuration. Then, run terraform init. Terraform 0.13+ terraform { required_providers { harvester = { source = \"harvester/harvester\" version = \"0.2.9\" } } } provider \"harvester\" { # Configuration options } For more details, please refer to the Harvester provider documentation . Option 2: Build and install the provider manually. \u00b6 Building the provider: \u00b6 Clone the repository using the following command: git clone git@github.com:harvester/terraform-provider-harvester Enter the provider directory and build the provider; this will build the provider and put the provider binary in ./bin . Use the following command: cd terraform-provider-harvester make Installing the provider: \u00b6 The expected location for the Harvester provider for the target platform within one of the local search directories is as follows: registry.terraform.io/harvester/harvester/0.2.9/linux_amd64/terraform-provider-harvester_v0.2.9 The default location for locally-installed providers will be one of the following, depending on the operating system under which you are running Terraform: Windows: %APPDATA%\\terraform.d\\plugins All other systems: ~/.terraform.d/plugins Place the provider into the plugin directory as in the following example: version=0.2.9 arch=linux_amd64 terraform_harvester_provider_bin=./bin/terraform-provider-harvester terraform_harvester_provider_dir=\"${HOME}/.terraform.d/plugins/registry.terraform.io/harvester/harvester/${version}/${arch}/\" mkdir -p \"${terraform_harvester_provider_dir}\" cp ${terraform_harvester_provider_bin} \"${terraform_harvester_provider_dir}/terraform-provider-harvester_v${version}\"} Using the provider \u00b6 After placing the provider into your plugins directory, run terraform init to initialize it. More information about provider-specific configuration options can be found on the docs directory","title":"Harvester Terraform Provider"},{"location":"terraform/terraform/#harvester-terraform-provider","text":"","title":"Harvester Terraform Provider"},{"location":"terraform/terraform/#requirements","text":"Terraform >= 0.13.x Go 1.16 to build the provider plugin","title":"Requirements"},{"location":"terraform/terraform/#install-the-provider","text":"","title":"Install the Provider"},{"location":"terraform/terraform/#option-1-download-and-install-the-provider-from-the-terraform-registry","text":"To install this provider, copy and paste this code into your Terraform configuration. Then, run terraform init. Terraform 0.13+ terraform { required_providers { harvester = { source = \"harvester/harvester\" version = \"0.2.9\" } } } provider \"harvester\" { # Configuration options } For more details, please refer to the Harvester provider documentation .","title":"Option 1: Download and install the provider from the Terraform registry."},{"location":"terraform/terraform/#option-2-build-and-install-the-provider-manually","text":"","title":"Option 2: Build and install the provider manually."},{"location":"terraform/terraform/#building-the-provider","text":"Clone the repository using the following command: git clone git@github.com:harvester/terraform-provider-harvester Enter the provider directory and build the provider; this will build the provider and put the provider binary in ./bin . Use the following command: cd terraform-provider-harvester make","title":"Building the provider:"},{"location":"terraform/terraform/#installing-the-provider","text":"The expected location for the Harvester provider for the target platform within one of the local search directories is as follows: registry.terraform.io/harvester/harvester/0.2.9/linux_amd64/terraform-provider-harvester_v0.2.9 The default location for locally-installed providers will be one of the following, depending on the operating system under which you are running Terraform: Windows: %APPDATA%\\terraform.d\\plugins All other systems: ~/.terraform.d/plugins Place the provider into the plugin directory as in the following example: version=0.2.9 arch=linux_amd64 terraform_harvester_provider_bin=./bin/terraform-provider-harvester terraform_harvester_provider_dir=\"${HOME}/.terraform.d/plugins/registry.terraform.io/harvester/harvester/${version}/${arch}/\" mkdir -p \"${terraform_harvester_provider_dir}\" cp ${terraform_harvester_provider_bin} \"${terraform_harvester_provider_dir}/terraform-provider-harvester_v${version}\"}","title":"Installing the provider:"},{"location":"terraform/terraform/#using-the-provider","text":"After placing the provider into your plugins directory, run terraform init to initialize it. More information about provider-specific configuration options can be found on the docs directory","title":"Using the provider"},{"location":"troubleshooting/harvester/","text":"Generate a support bundle \u00b6 Users can generate a support bundle in the Harvester GUI with the following steps: Click the Support link at the bottom-left of Harvester Web UI. Click Generate Support Bundle button. Enter a useful description for the support bundle and click Create to generate and download a support bundle. Access Embedded Rancher \u00b6 You can access the embedded Rancher dashboard via https://{{HARVESTER_IP}}/dashboard/c/local/explorer . Note We only support to use the embedded Rancher dashboard for debugging and validation purpose. For Rancher's multi-cluster and multi-tenant integration, please refer to the docs here . Access Embedded Longhorn \u00b6 You can access the embedded Longhorn UI via https://{{HARVESTER_IP}}/dashboard/c/local/longhorn . Note We only support to use the embedded Longhorn UI for debugging and validation purpose . I can't access Harvester after I changed SSL/TLS enabled protocols and ciphers \u00b6 If you changed SSL/TLS enabled protocols and ciphers settings and you no longer have access to Harvester GUI and API, it's highly possible that NGINX Ingress Controller has stopped working due to the misconfigured SSL/TLS protocols and ciphers. Follow these steps to reset the setting: Following FAQ to SSH into Harvester node and switch to root user. $ sudo -s Editing setting ssl-parameters manually using kubectl : # kubectl edit settings ssl-parameters Deleting the line value: ... so that NGINX Ingress Controller will use the default protocols and ciphers. apiVersion: harvesterhci.io/v1beta1 default: '{}' kind: Setting metadata: name: ssl-parameters ... value: '{\"protocols\":\"TLS99\",\"ciphers\":\"WRONG_CIPHER\"}' # <- Delete this line Save the change and you should see the following response after exit from the editor: setting.harvesterhci.io/ssl-parameters edited You can further check the logs of Pod rke2-ingress-nginx-controller to see if NGINX Ingress Controller is working correctly.","title":"Harvester"},{"location":"troubleshooting/harvester/#generate-a-support-bundle","text":"Users can generate a support bundle in the Harvester GUI with the following steps: Click the Support link at the bottom-left of Harvester Web UI. Click Generate Support Bundle button. Enter a useful description for the support bundle and click Create to generate and download a support bundle.","title":"Generate a support bundle"},{"location":"troubleshooting/harvester/#access-embedded-rancher","text":"You can access the embedded Rancher dashboard via https://{{HARVESTER_IP}}/dashboard/c/local/explorer . Note We only support to use the embedded Rancher dashboard for debugging and validation purpose. For Rancher's multi-cluster and multi-tenant integration, please refer to the docs here .","title":"Access Embedded Rancher"},{"location":"troubleshooting/harvester/#access-embedded-longhorn","text":"You can access the embedded Longhorn UI via https://{{HARVESTER_IP}}/dashboard/c/local/longhorn . Note We only support to use the embedded Longhorn UI for debugging and validation purpose .","title":"Access Embedded Longhorn"},{"location":"troubleshooting/harvester/#i-cant-access-harvester-after-i-changed-ssltls-enabled-protocols-and-ciphers","text":"If you changed SSL/TLS enabled protocols and ciphers settings and you no longer have access to Harvester GUI and API, it's highly possible that NGINX Ingress Controller has stopped working due to the misconfigured SSL/TLS protocols and ciphers. Follow these steps to reset the setting: Following FAQ to SSH into Harvester node and switch to root user. $ sudo -s Editing setting ssl-parameters manually using kubectl : # kubectl edit settings ssl-parameters Deleting the line value: ... so that NGINX Ingress Controller will use the default protocols and ciphers. apiVersion: harvesterhci.io/v1beta1 default: '{}' kind: Setting metadata: name: ssl-parameters ... value: '{\"protocols\":\"TLS99\",\"ciphers\":\"WRONG_CIPHER\"}' # <- Delete this line Save the change and you should see the following response after exit from the editor: setting.harvesterhci.io/ssl-parameters edited You can further check the logs of Pod rke2-ingress-nginx-controller to see if NGINX Ingress Controller is working correctly.","title":"I can't access Harvester after I changed SSL/TLS enabled protocols and ciphers"},{"location":"troubleshooting/installation/","text":"Installation \u00b6 The following sections contain tips to troubleshoot or get assistance with failed installations. Logging into the Harvester Installer (a live OS) \u00b6 Users can press the key combination CTRL + ALT + F2 to switch to another TTY and log in with the following credentials: User: rancher Password: rancher Meeting hardware requirements \u00b6 Check that your hardware meets the minimum requirements to complete installation. Receiving the message \"Loading images. This may take a few minutes...\" \u00b6 Because the system doesn't have a default route, your installer may become \"stuck\" in this state. You can check your route status by executing the following command: $ ip route default via 10.10.0.10 dev harvester-mgmt proto dhcp <-- Does a default route exist? 10.10.0.0/24 dev harvester-mgmt proto kernel scope link src 10.10.0.15 Check that your DHCP server offers a default route option. Attaching content from /run/cos/target/rke2.log is helpful too. Modifying cluster token on agent nodes \u00b6 When an agent node fails to join the cluster, it can be related to the cluster token not being identical to the server node token. In order to confirm the issue, connect to your agent node (i.e. with SSH ) and check the rancherd service log with the following command: $ sudo journalctl -b -u rancherd If the cluster token setup in the agent node is not matching the server node token, you will find several entries of the following message: ... msg=\"Bootstrapping Rancher (master-head/v1.21.5+rke2r1)\" msg=\"failed to bootstrap system, will retry: generating plan: insecure cacerts download from https://192.168.122.115:8443/cacerts: Get \\\"https://192.168.122.115:8443/cacerts\\\": EOF\" ... To fix the issue, you need to update the token value in the rancherd configuration file /etc/rancher/rancherd/config.yaml . For example, if the cluster token setup in the server node is ThisIsTheCorrectOne , you will update the token value as follow: ... token : 'ThisIsTheCorrectOne' ... To ensure the change is persistent across reboots, update the token value of the OS configuration file /oem/99_custom.yaml : name : Harvester Configuration stages : ... initramfs : - commands : - rm -f /etc/sysconfig/network/ifroute-harvester-mgmt files : - path : /etc/rancher/rancherd/config.yaml permissions : 384 owner : 0 group : 0 content : | role: cluster-init token: 'ThisIsTheCorrectOne' # <- Update this value kubernetesVersion: v1.21.5+rke2r1 labels: - harvesterhci.io/managed=true encoding : \"\" ownerstring : \"\" Note To see what is the current cluster token value, log in your server node (i.e. with SSH) and look in the file /etc/rancher/rancherd/config.yaml . For example, you can run the following command to only display the token's value: $ sudo yq eval .token /etc/rancher/rancherd/config.yaml Collecting information \u00b6 Please include the following information in a bug report when reporting a failed installation: A failed installation screenshot. Content of these files: /var/log/console.log /run/cos/target/rke2.log /tmp/harvester.* /tmp/cos.* Output of these commands: blkid dmesg","title":"Installation"},{"location":"troubleshooting/installation/#installation","text":"The following sections contain tips to troubleshoot or get assistance with failed installations.","title":"Installation"},{"location":"troubleshooting/installation/#logging-into-the-harvester-installer-a-live-os","text":"Users can press the key combination CTRL + ALT + F2 to switch to another TTY and log in with the following credentials: User: rancher Password: rancher","title":"Logging into the Harvester Installer (a live OS)"},{"location":"troubleshooting/installation/#meeting-hardware-requirements","text":"Check that your hardware meets the minimum requirements to complete installation.","title":"Meeting hardware requirements"},{"location":"troubleshooting/installation/#receiving-the-message-loading-images-this-may-take-a-few-minutes","text":"Because the system doesn't have a default route, your installer may become \"stuck\" in this state. You can check your route status by executing the following command: $ ip route default via 10.10.0.10 dev harvester-mgmt proto dhcp <-- Does a default route exist? 10.10.0.0/24 dev harvester-mgmt proto kernel scope link src 10.10.0.15 Check that your DHCP server offers a default route option. Attaching content from /run/cos/target/rke2.log is helpful too.","title":"Receiving the message \"Loading images. This may take a few minutes...\""},{"location":"troubleshooting/installation/#modifying-cluster-token-on-agent-nodes","text":"When an agent node fails to join the cluster, it can be related to the cluster token not being identical to the server node token. In order to confirm the issue, connect to your agent node (i.e. with SSH ) and check the rancherd service log with the following command: $ sudo journalctl -b -u rancherd If the cluster token setup in the agent node is not matching the server node token, you will find several entries of the following message: ... msg=\"Bootstrapping Rancher (master-head/v1.21.5+rke2r1)\" msg=\"failed to bootstrap system, will retry: generating plan: insecure cacerts download from https://192.168.122.115:8443/cacerts: Get \\\"https://192.168.122.115:8443/cacerts\\\": EOF\" ... To fix the issue, you need to update the token value in the rancherd configuration file /etc/rancher/rancherd/config.yaml . For example, if the cluster token setup in the server node is ThisIsTheCorrectOne , you will update the token value as follow: ... token : 'ThisIsTheCorrectOne' ... To ensure the change is persistent across reboots, update the token value of the OS configuration file /oem/99_custom.yaml : name : Harvester Configuration stages : ... initramfs : - commands : - rm -f /etc/sysconfig/network/ifroute-harvester-mgmt files : - path : /etc/rancher/rancherd/config.yaml permissions : 384 owner : 0 group : 0 content : | role: cluster-init token: 'ThisIsTheCorrectOne' # <- Update this value kubernetesVersion: v1.21.5+rke2r1 labels: - harvesterhci.io/managed=true encoding : \"\" ownerstring : \"\" Note To see what is the current cluster token value, log in your server node (i.e. with SSH) and look in the file /etc/rancher/rancherd/config.yaml . For example, you can run the following command to only display the token's value: $ sudo yq eval .token /etc/rancher/rancherd/config.yaml","title":"Modifying cluster token on agent nodes"},{"location":"troubleshooting/installation/#collecting-information","text":"Please include the following information in a bug report when reporting a failed installation: A failed installation screenshot. Content of these files: /var/log/console.log /run/cos/target/rke2.log /tmp/harvester.* /tmp/cos.* Output of these commands: blkid dmesg","title":"Collecting information"},{"location":"troubleshooting/os/","text":"Operating System \u00b6 Harvester runs on an OpenSUSE-based OS. The OS is an artifact produced by the cOS toolkit . The following sections contain information and tips to help users troubleshoot OS-related issues. How to log into a Harvester node \u00b6 Users can log into a Harvester node with the username rancher and the password or SSH keypair provided during installation. The user rancher can execute privileged commands without entering a password: # Run a privileged command rancher@node1:~> sudo blkid # Or become root rancher@node1:~> sudo -i node1:~ # blkid How can I install packages? Why are some paths read-only? \u00b6 The OS file system, like a container image, is image-based and immutable except in some directories. To temporarily enable the read-write mode, please use the following steps: Warning Enabling read-write mode might break your system if files are modified. Please use it at your own risk. For version v0.3.0 , we need to apply a workaround first to make some directories non-overlaid after enabling read-write mode. On a running Harvester node, run the following command as root: cat > /oem/91_hack.yaml <<'EOF' name: \"Rootfs Layout Settings for debugrw\" stages: rootfs: - if: 'grep -q root=LABEL=COS_ACTIVE /proc/cmdline && grep -q rd.cos.debugrw /proc/cmdline' name: \"Layout configuration for debugrw\" environment_file: /run/cos/cos-layout.env environment: RW_PATHS: \" \" EOF Reboot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append rd.cos.debugrw to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. How to permanently edit kernel parameters \u00b6 Note The following steps are a workaround. Harvester will inform the community once a permanent resolution is in place. Re-mount state directory in rw mode: # blkid -L COS_STATE /dev/vda2 # mount -o remount,rw /dev/vda2 /run/initramfs/cos-state Edit the grub config file and append parameters to the linux (loop0)$kernel $kernelcmd line. The following example adds a nomodeset parameter: # vim /run/initramfs/cos-state/grub2/grub.cfg menuentry \"Harvester ea6e7f5-dirty\" --id cos { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd nomodeset initrd (loop0)$initramfs } Reboot for changes to take effect. How to change the default GRUB boot menu entry \u00b6 To change the default entry, first check the --id attribute of a menu entry, as in the following example: # cat /run/initramfs/cos-state/grub2/grub.cfg <...> menuentry \"Harvester ea6e7f5-dirty (debug)\" --id cos-debug { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img The id of the above entry is cos-debug . We can then set the default entry by: # grub2-editenv /oem/grubenv set saved_entry=cos-debug How to debug a system crash or hang \u00b6 Collect crash log \u00b6 If kernel panic traces are not recorded in the system log when a system crashes, one reliable way to locate the crash log is to use a serial console. To enable outputting of kernel messages to a serial console, please use the following steps: Boot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append console=ttyS0,115200n8 to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. Note Adjust the console options according to your environment. Make sure to append the console= string at the end of the line. Connect to the serial port to capture logs. Collect crash dumps \u00b6 For kernel panic crashes, you can use kdump to collect crash dumps. By default, the OS is booted without the kdump feature enabled. Users can enable the feature by selecting the debug menuentry when booting, as in the following example: When a system crashes, a crash dump will be stored in the /var/crash/<time> directory. Providing the crash dump to developers helps them to troubleshoot and resolve issues.","title":"Operating System"},{"location":"troubleshooting/os/#operating-system","text":"Harvester runs on an OpenSUSE-based OS. The OS is an artifact produced by the cOS toolkit . The following sections contain information and tips to help users troubleshoot OS-related issues.","title":"Operating System"},{"location":"troubleshooting/os/#how-to-log-into-a-harvester-node","text":"Users can log into a Harvester node with the username rancher and the password or SSH keypair provided during installation. The user rancher can execute privileged commands without entering a password: # Run a privileged command rancher@node1:~> sudo blkid # Or become root rancher@node1:~> sudo -i node1:~ # blkid","title":"How to log into a Harvester node"},{"location":"troubleshooting/os/#how-can-i-install-packages-why-are-some-paths-read-only","text":"The OS file system, like a container image, is image-based and immutable except in some directories. To temporarily enable the read-write mode, please use the following steps: Warning Enabling read-write mode might break your system if files are modified. Please use it at your own risk. For version v0.3.0 , we need to apply a workaround first to make some directories non-overlaid after enabling read-write mode. On a running Harvester node, run the following command as root: cat > /oem/91_hack.yaml <<'EOF' name: \"Rootfs Layout Settings for debugrw\" stages: rootfs: - if: 'grep -q root=LABEL=COS_ACTIVE /proc/cmdline && grep -q rd.cos.debugrw /proc/cmdline' name: \"Layout configuration for debugrw\" environment_file: /run/cos/cos-layout.env environment: RW_PATHS: \" \" EOF Reboot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append rd.cos.debugrw to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system.","title":"How can I install packages? Why are some paths read-only?"},{"location":"troubleshooting/os/#how-to-permanently-edit-kernel-parameters","text":"Note The following steps are a workaround. Harvester will inform the community once a permanent resolution is in place. Re-mount state directory in rw mode: # blkid -L COS_STATE /dev/vda2 # mount -o remount,rw /dev/vda2 /run/initramfs/cos-state Edit the grub config file and append parameters to the linux (loop0)$kernel $kernelcmd line. The following example adds a nomodeset parameter: # vim /run/initramfs/cos-state/grub2/grub.cfg menuentry \"Harvester ea6e7f5-dirty\" --id cos { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd nomodeset initrd (loop0)$initramfs } Reboot for changes to take effect.","title":"How to permanently edit kernel parameters"},{"location":"troubleshooting/os/#how-to-change-the-default-grub-boot-menu-entry","text":"To change the default entry, first check the --id attribute of a menu entry, as in the following example: # cat /run/initramfs/cos-state/grub2/grub.cfg <...> menuentry \"Harvester ea6e7f5-dirty (debug)\" --id cos-debug { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img The id of the above entry is cos-debug . We can then set the default entry by: # grub2-editenv /oem/grubenv set saved_entry=cos-debug","title":"How to change the default GRUB boot menu entry"},{"location":"troubleshooting/os/#how-to-debug-a-system-crash-or-hang","text":"","title":"How to debug a system crash or hang"},{"location":"troubleshooting/os/#collect-crash-log","text":"If kernel panic traces are not recorded in the system log when a system crashes, one reliable way to locate the crash log is to use a serial console. To enable outputting of kernel messages to a serial console, please use the following steps: Boot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append console=ttyS0,115200n8 to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. Note Adjust the console options according to your environment. Make sure to append the console= string at the end of the line. Connect to the serial port to capture logs.","title":"Collect crash log"},{"location":"troubleshooting/os/#collect-crash-dumps","text":"For kernel panic crashes, you can use kdump to collect crash dumps. By default, the OS is booted without the kdump feature enabled. Users can enable the feature by selecting the debug menuentry when booting, as in the following example: When a system crashes, a crash dump will be stored in the /var/crash/<time> directory. Providing the crash dump to developers helps them to troubleshoot and resolve issues.","title":"Collect crash dumps"},{"location":"vm/access-to-the-vm/","text":"Access to the Virtual Machine (VM) \u00b6 Once the VM is up and running, you can access it using either the Virtual Network Computing (VNC) client or the serial console from the Harvester UI. Additionally, you can connect directly from your computer's SSH client. Access with the Harvester UI \u00b6 VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM, as with the Ubuntu minimal cloud image, the VM can be accessed with the serial console. Access with the SSH Client \u00b6 Enter the IP address of the host in a terminal emulation client, such as PuTTY. You may also run the following command to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@<ip-address-or-hostname>","title":"Access to the Virtual Machine (VM)"},{"location":"vm/access-to-the-vm/#access-to-the-virtual-machine-vm","text":"Once the VM is up and running, you can access it using either the Virtual Network Computing (VNC) client or the serial console from the Harvester UI. Additionally, you can connect directly from your computer's SSH client.","title":"Access to the Virtual Machine (VM)"},{"location":"vm/access-to-the-vm/#access-with-the-harvester-ui","text":"VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM, as with the Ubuntu minimal cloud image, the VM can be accessed with the serial console.","title":"Access with the Harvester UI"},{"location":"vm/access-to-the-vm/#access-with-the-ssh-client","text":"Enter the IP address of the host in a terminal emulation client, such as PuTTY. You may also run the following command to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@<ip-address-or-hostname>","title":"Access with the SSH Client"},{"location":"vm/backup-restore/","text":"VM Backup & Restore \u00b6 Available as of v0.3.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server), and they can be used to either restore a new VM or replace an existing VM. Note A backup target must be set up. For more information, see Configure Backup Target . If the backup target has not been set, you\u2019ll be prompted with a message to do so. Configure Backup Target \u00b6 A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings > backup-target . Parameter Type Description Type string Choose S3 or NFS Endpoint string A hostname or an IP address. It can be left empty for AWS S3. BucketName string Name of the bucket BucketRegion string Region of the bucket AccessKeyID string A user-id that uniquely identifies your account SecretAccessKey string The password to your account Certificate string Paste to use a self-signed SSL certificate of your S3 server VirtualHostedStyle bool Use VirtualHostedStyle access only; e.g., Alibaba Cloud (Aliyun) OSS Create a VM backup \u00b6 Once the backup target is set, go to the Virtual Machines page. Click Take Backup of the VM actions to create a new VM backup. Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. You will receive a notification message, and you can also go to the Advanced > Backups page to view all VM backups. The ReadyToUse status will be set to true once the Backup is complete. Users can either choose to restore a new VM or replace an existing VM using this backup. Restore a new VM using a backup \u00b6 To restore a new VM from a backup, follow these steps: Go to the Backups page. Specify the new VM name and click Create . A new VM will be restored using the backup volumes and metadata, and you can access it from the Virtual Machines page. Replace an Existing VM using a backup \u00b6 You can replace an existing VM using the backup with the same VM backup target. You can choose to either delete or retain the previous volumes. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the Backups page. Click Create . The restore process can be viewed from the Virtual Machines page. Restore a new VM on another Harvester cluster \u00b6 Available as of v1.0.0 Users can now restore a new VM on another cluster by leveraging the VM metadata & content backup feature. Prerequisites You must manually configure the virtual machine images with the same name on the new cluster first, otherwise the virtual machines will be failed to recover. Upload the same VM images to a new cluster \u00b6 Check the existing image name (normally starts with image- ) and create the same one on the new cluster. $ kubectl get vmimages -A NAMESPACE NAME DISPLAY-NAME SIZE AGE default image-79hdq focal-server-cloudimg-amd64.img 566886400 5h36m default image-l7924 harvester-v1.0.0-rc2-amd64.iso 3964551168 137m default image-lvqxn opensuse-leap-15.3.x86_64-nocloud.qcow2 568524800 5h35m Apply a VM image YAML with the same name and content in the new cluster. $ cat <<EOF | kubectl apply -f - apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: name: image-lvqxn namespace: default spec: displayName: opensuse-leap-15.3.x86_64-nocloud.qcow2 pvcName: \"\" pvcNamespace: \"\" sourceType: download url: http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2 EOF Restore a new VM in a new cluster \u00b6 Setup the same backup target in a new cluster. And the backup controller will automatically sync the backup metadata to the new cluster. Go to the Backups page. Select the synced VM backup metadata and choose to restore a new VM with a specified VM name. A new VM will be restored using the backup volumes and metadata. You can access it from the Virtual Machines page.","title":"VM Backup & Restore"},{"location":"vm/backup-restore/#vm-backup-restore","text":"Available as of v0.3.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server), and they can be used to either restore a new VM or replace an existing VM. Note A backup target must be set up. For more information, see Configure Backup Target . If the backup target has not been set, you\u2019ll be prompted with a message to do so.","title":"VM Backup &amp; Restore"},{"location":"vm/backup-restore/#configure-backup-target","text":"A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings > backup-target . Parameter Type Description Type string Choose S3 or NFS Endpoint string A hostname or an IP address. It can be left empty for AWS S3. BucketName string Name of the bucket BucketRegion string Region of the bucket AccessKeyID string A user-id that uniquely identifies your account SecretAccessKey string The password to your account Certificate string Paste to use a self-signed SSL certificate of your S3 server VirtualHostedStyle bool Use VirtualHostedStyle access only; e.g., Alibaba Cloud (Aliyun) OSS","title":"Configure Backup Target"},{"location":"vm/backup-restore/#create-a-vm-backup","text":"Once the backup target is set, go to the Virtual Machines page. Click Take Backup of the VM actions to create a new VM backup. Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. You will receive a notification message, and you can also go to the Advanced > Backups page to view all VM backups. The ReadyToUse status will be set to true once the Backup is complete. Users can either choose to restore a new VM or replace an existing VM using this backup.","title":"Create a VM backup"},{"location":"vm/backup-restore/#restore-a-new-vm-using-a-backup","text":"To restore a new VM from a backup, follow these steps: Go to the Backups page. Specify the new VM name and click Create . A new VM will be restored using the backup volumes and metadata, and you can access it from the Virtual Machines page.","title":"Restore a new VM using a backup"},{"location":"vm/backup-restore/#replace-an-existing-vm-using-a-backup","text":"You can replace an existing VM using the backup with the same VM backup target. You can choose to either delete or retain the previous volumes. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the Backups page. Click Create . The restore process can be viewed from the Virtual Machines page.","title":"Replace an Existing VM using a backup"},{"location":"vm/backup-restore/#restore-a-new-vm-on-another-harvester-cluster","text":"Available as of v1.0.0 Users can now restore a new VM on another cluster by leveraging the VM metadata & content backup feature. Prerequisites You must manually configure the virtual machine images with the same name on the new cluster first, otherwise the virtual machines will be failed to recover.","title":"Restore a new VM on another Harvester cluster"},{"location":"vm/backup-restore/#upload-the-same-vm-images-to-a-new-cluster","text":"Check the existing image name (normally starts with image- ) and create the same one on the new cluster. $ kubectl get vmimages -A NAMESPACE NAME DISPLAY-NAME SIZE AGE default image-79hdq focal-server-cloudimg-amd64.img 566886400 5h36m default image-l7924 harvester-v1.0.0-rc2-amd64.iso 3964551168 137m default image-lvqxn opensuse-leap-15.3.x86_64-nocloud.qcow2 568524800 5h35m Apply a VM image YAML with the same name and content in the new cluster. $ cat <<EOF | kubectl apply -f - apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: name: image-lvqxn namespace: default spec: displayName: opensuse-leap-15.3.x86_64-nocloud.qcow2 pvcName: \"\" pvcNamespace: \"\" sourceType: download url: http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2 EOF","title":"Upload the same VM images to a new cluster"},{"location":"vm/backup-restore/#restore-a-new-vm-in-a-new-cluster","text":"Setup the same backup target in a new cluster. And the backup controller will automatically sync the backup metadata to the new cluster. Go to the Backups page. Select the synced VM backup metadata and choose to restore a new VM with a specified VM name. A new VM will be restored using the backup volumes and metadata. You can access it from the Virtual Machines page.","title":"Restore a new VM in a new cluster"},{"location":"vm/create-vm/","text":"Create a VM \u00b6 How to Create a VM \u00b6 Create one or more virtual machines from the Virtual Machines page. Note For creating Windows virtual machines, please refer to this page . Choose the option to create either one or multiple VM instances. The VM Name is a required field. The VM Template is optional. You can select ISO, raw, and Windows image templates as default options. Configure the CPU and Memory of the VM. Select SSH keys or upload new keys. Select a custom VM image on the Volumes tab. The default disk will be the root disk. You can add more disks to the VM. To configure networks, go to the Networks tab. The Management Network is added by default. It is also possible to add secondary networks to the VMs using VLAN networks. You may configure these on Advanced > Networks . Advanced options such as hostname and cloud-init data are optional. You may configure these in the Advanced Options section. Cloud Configuration Examples \u00b6 Password configuration for the default user: # cloud-config password : password chpasswd : { expire : False } ssh_pwauth : True Network-data configuration using DHCP: version : 1 config : - type : physical name : eth0 subnets : - type : dhcp - type : physical name : eth1 subnets : - type : dhcp You can also use the Cloud Config Template feature to include a pre-defined cloud-init configuration for the VM. Networks \u00b6 Management Network \u00b6 A management network represents the default VM eth0 interface configured by the cluster network solution that is present in each VM. By default, a VM can be accessed via the management network. Secondary Network \u00b6 It is also possible to connect VMs using additional networks with Harvester's built-in VLAN networks .","title":"Create a VM"},{"location":"vm/create-vm/#create-a-vm","text":"","title":"Create a VM"},{"location":"vm/create-vm/#how-to-create-a-vm","text":"Create one or more virtual machines from the Virtual Machines page. Note For creating Windows virtual machines, please refer to this page . Choose the option to create either one or multiple VM instances. The VM Name is a required field. The VM Template is optional. You can select ISO, raw, and Windows image templates as default options. Configure the CPU and Memory of the VM. Select SSH keys or upload new keys. Select a custom VM image on the Volumes tab. The default disk will be the root disk. You can add more disks to the VM. To configure networks, go to the Networks tab. The Management Network is added by default. It is also possible to add secondary networks to the VMs using VLAN networks. You may configure these on Advanced > Networks . Advanced options such as hostname and cloud-init data are optional. You may configure these in the Advanced Options section.","title":"How to Create a VM"},{"location":"vm/create-vm/#cloud-configuration-examples","text":"Password configuration for the default user: # cloud-config password : password chpasswd : { expire : False } ssh_pwauth : True Network-data configuration using DHCP: version : 1 config : - type : physical name : eth0 subnets : - type : dhcp - type : physical name : eth1 subnets : - type : dhcp You can also use the Cloud Config Template feature to include a pre-defined cloud-init configuration for the VM.","title":"Cloud Configuration Examples"},{"location":"vm/create-vm/#networks","text":"","title":"Networks"},{"location":"vm/create-vm/#management-network","text":"A management network represents the default VM eth0 interface configured by the cluster network solution that is present in each VM. By default, a VM can be accessed via the management network.","title":"Management Network"},{"location":"vm/create-vm/#secondary-network","text":"It is also possible to connect VMs using additional networks with Harvester's built-in VLAN networks .","title":"Secondary Network"},{"location":"vm/create-windows-vm/","text":"Create a Windows VM \u00b6 Create one or more virtual machines from the Virtual Machines page. Note For creating Linux virtual machines, please refer to this page . Header Section \u00b6 Create a single VM instance or multiple VM instances. Set the VM name. [Optional] Provide a description for the VM. [Optional] Select the VM template windows-iso-image-base-template . This template will add a volume with the virtio drivers for Windows. Basics Tab \u00b6 Configure the number of CPU cores assigned to the VM. Configure the amount of Memory assigned to the VM. [Optional] Select existing SSH keys or upload new ones. Note As mentioned above, it is recommended that you use the Windows VM template. The Volumes section will describe the options which the Windows VM template created automatically. Warning The bootOrder values need to be set with the installation image first. If you change it, your VM might not boot into the installation disk. Volumes Tab \u00b6 The first volume is an Image Volume with the following values: Name : The value cdrom-disk is set by default. You can keep it or change it. Image : Select the Windows image to be installed. See Upload Images for the full description on how to create new images. Type : Select cd-rom . Size : The value 20 is set by default. You can change it if your image has a bigger size. Bus : The value SATA is set by default. It's recommended you don't change it. The second volume is a Volume with the following values: Name : The value rootdisk is set by default. You can keep it or change it. Size : The value 32 is set by default. See the disk space requirements for Windows Server and Windows 11 before changing this value. Bus : The value VirtIO is set by default. You can keep it or change it to the other available options, SATA or SCSI . The third volume is a Container with the following values: Name : The value virtio-container-disk is set by default. You can keep it or change it. Docker Image : The value registry.suse.com/harvester-beta/vmdp:latest is set by default. It's recommended you don't change it. Bus : The value SATA is set by default. It's recommended you don't change it. You can add additional disks using the buttons Add Volume , Add Existing Volume , Add VM Image , or Add Container . Networks Tab \u00b6 The Management Network is added by default with the following values: Name : The value default is set by default. You can keep it or change it. Network : The value management Network is set by default. You can't change this option if no other network has been created. See Harvester Network for the full description on how to create new networks. Model : The value e1000 is set by default. You can keep it or change it to the other available options from the dropdown. Type : The value masquerade is set by default. You can keep it or change it to the other available option, bridge . You can add additional networks by clicking Add Network . Warning Changing the Node Scheduling settings can impact Harvester features, such as disabling Live migration . Node Scheduling Tab \u00b6 Node Scheduling is set to Run VM on any available node by default. You can keep it or change it to the other available options from the dropdown. Advanced Options Tab \u00b6 OS Type : The value Windows is set by default. It's recommended you don't change it. Machine Type : The value None is set by default. It's recommended you don't change it. See the KubeVirt Machine Type documentation before you change this value. [Optional] Hostname : Set the VM hostname. [Optional] Cloud Config : Both User Data and Network Data values are set with default values. Currently, these configurations are not applied to Windows-based VMs. Footer Section \u00b6 Start virtual machine on creation : This option is checked by default. You can uncheck it if you don't want the VM to start once it's created. Once all the settings are in place, click on Create to create the VM. Note If you need to add advanced settings, you can edit the VM configuration directly by clicking on Edit as YAML . And if you want to cancel all changes made, click Cancel .","title":"Create a Windows VM"},{"location":"vm/create-windows-vm/#create-a-windows-vm","text":"Create one or more virtual machines from the Virtual Machines page. Note For creating Linux virtual machines, please refer to this page .","title":"Create a Windows VM"},{"location":"vm/create-windows-vm/#header-section","text":"Create a single VM instance or multiple VM instances. Set the VM name. [Optional] Provide a description for the VM. [Optional] Select the VM template windows-iso-image-base-template . This template will add a volume with the virtio drivers for Windows.","title":"Header Section"},{"location":"vm/create-windows-vm/#basics-tab","text":"Configure the number of CPU cores assigned to the VM. Configure the amount of Memory assigned to the VM. [Optional] Select existing SSH keys or upload new ones. Note As mentioned above, it is recommended that you use the Windows VM template. The Volumes section will describe the options which the Windows VM template created automatically. Warning The bootOrder values need to be set with the installation image first. If you change it, your VM might not boot into the installation disk.","title":"Basics Tab"},{"location":"vm/create-windows-vm/#volumes-tab","text":"The first volume is an Image Volume with the following values: Name : The value cdrom-disk is set by default. You can keep it or change it. Image : Select the Windows image to be installed. See Upload Images for the full description on how to create new images. Type : Select cd-rom . Size : The value 20 is set by default. You can change it if your image has a bigger size. Bus : The value SATA is set by default. It's recommended you don't change it. The second volume is a Volume with the following values: Name : The value rootdisk is set by default. You can keep it or change it. Size : The value 32 is set by default. See the disk space requirements for Windows Server and Windows 11 before changing this value. Bus : The value VirtIO is set by default. You can keep it or change it to the other available options, SATA or SCSI . The third volume is a Container with the following values: Name : The value virtio-container-disk is set by default. You can keep it or change it. Docker Image : The value registry.suse.com/harvester-beta/vmdp:latest is set by default. It's recommended you don't change it. Bus : The value SATA is set by default. It's recommended you don't change it. You can add additional disks using the buttons Add Volume , Add Existing Volume , Add VM Image , or Add Container .","title":"Volumes Tab"},{"location":"vm/create-windows-vm/#networks-tab","text":"The Management Network is added by default with the following values: Name : The value default is set by default. You can keep it or change it. Network : The value management Network is set by default. You can't change this option if no other network has been created. See Harvester Network for the full description on how to create new networks. Model : The value e1000 is set by default. You can keep it or change it to the other available options from the dropdown. Type : The value masquerade is set by default. You can keep it or change it to the other available option, bridge . You can add additional networks by clicking Add Network . Warning Changing the Node Scheduling settings can impact Harvester features, such as disabling Live migration .","title":"Networks Tab"},{"location":"vm/create-windows-vm/#node-scheduling-tab","text":"Node Scheduling is set to Run VM on any available node by default. You can keep it or change it to the other available options from the dropdown.","title":"Node Scheduling Tab"},{"location":"vm/create-windows-vm/#advanced-options-tab","text":"OS Type : The value Windows is set by default. It's recommended you don't change it. Machine Type : The value None is set by default. It's recommended you don't change it. See the KubeVirt Machine Type documentation before you change this value. [Optional] Hostname : Set the VM hostname. [Optional] Cloud Config : Both User Data and Network Data values are set with default values. Currently, these configurations are not applied to Windows-based VMs.","title":"Advanced Options Tab"},{"location":"vm/create-windows-vm/#footer-section","text":"Start virtual machine on creation : This option is checked by default. You can uncheck it if you don't want the VM to start once it's created. Once all the settings are in place, click on Create to create the VM. Note If you need to add advanced settings, you can edit the VM configuration directly by clicking on Edit as YAML . And if you want to cancel all changes made, click Cancel .","title":"Footer Section"},{"location":"vm/hotplug-volume/","text":"Hot-Plug Volumes \u00b6 Harvester supports adding hot-plug volumes to a running VM. Adding Hot-Plug Volumes to a Running VM \u00b6 The following steps assume that you have a running VM and a ready volume: Go to the Virtual Machines page. Find the VM that you want to add a volume to and select \u22ee > Add Volume . Enter the Name and select the Volume . Click Apply .","title":"Hot-Plug Volumes"},{"location":"vm/hotplug-volume/#hot-plug-volumes","text":"Harvester supports adding hot-plug volumes to a running VM.","title":"Hot-Plug Volumes"},{"location":"vm/hotplug-volume/#adding-hot-plug-volumes-to-a-running-vm","text":"The following steps assume that you have a running VM and a ready volume: Go to the Virtual Machines page. Find the VM that you want to add a volume to and select \u22ee > Add Volume . Enter the Name and select the Volume . Click Apply .","title":"Adding Hot-Plug Volumes to a Running VM"},{"location":"vm/live-migration/","text":"Live Migration \u00b6 Live migration means moving a virtual machine to a different host without downtime. Note Live migration is not allowed when the virtual machine is using a management network of bridge interface type. To support live migration, three or more hosts in the Harvester cluster are required due to a known issue . Starting a Migration \u00b6 Go to the Virtual Machines page. Find the virtual machine that you want to migrate and select \u22ee > Migrate . Choose the node to which you want to migrate the virtual machine. Click Apply . Aborting a Migration \u00b6 Go to the Virtual Machines page. Find the virtual machine in migrating status that you want to abort. Select \u22ee > Abort Migration . Migration Timeouts \u00b6 Completion Timeout \u00b6 The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages or disk blocks at a higher rate than these can be copied. As a result, the migration process is prevented from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout of 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds. Progress Timeout \u00b6 Live migration will also be aborted when copying memory doesn't make any progress in 150s.","title":"Live Migration"},{"location":"vm/live-migration/#live-migration","text":"Live migration means moving a virtual machine to a different host without downtime. Note Live migration is not allowed when the virtual machine is using a management network of bridge interface type. To support live migration, three or more hosts in the Harvester cluster are required due to a known issue .","title":"Live Migration"},{"location":"vm/live-migration/#starting-a-migration","text":"Go to the Virtual Machines page. Find the virtual machine that you want to migrate and select \u22ee > Migrate . Choose the node to which you want to migrate the virtual machine. Click Apply .","title":"Starting a Migration"},{"location":"vm/live-migration/#aborting-a-migration","text":"Go to the Virtual Machines page. Find the virtual machine in migrating status that you want to abort. Select \u22ee > Abort Migration .","title":"Aborting a Migration"},{"location":"vm/live-migration/#migration-timeouts","text":"","title":"Migration Timeouts"},{"location":"vm/live-migration/#completion-timeout","text":"The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages or disk blocks at a higher rate than these can be copied. As a result, the migration process is prevented from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout of 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds.","title":"Completion Timeout"},{"location":"vm/live-migration/#progress-timeout","text":"Live migration will also be aborted when copying memory doesn't make any progress in 150s.","title":"Progress Timeout"}]}